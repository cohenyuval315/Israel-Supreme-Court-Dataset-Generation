{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohen Yuval\n",
    "### ID : 208570184"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Of Content\n",
    "1. Data Scraping\n",
    "2. Dataset generation using NRE or any other algorithm , deep learning not allowed\n",
    "3. 2-5 subject tags for each verdict using itf-idf or any other algorithm\n",
    "4. 2-5 subject tags for each verdict using Word2Vec (same as 3)\n",
    "-  Compare (3) and (4)\n",
    "5. Dataset Statistics\n",
    "    - In Certain TimeFrame:\n",
    "    - a. Number of verdict of each judge\n",
    "    - b. Number of verdicts each judge gave in the top 10 popular subject tags\n",
    "6. Use RNN to perform NER recognision for dataset generation(2) \n",
    "7. - Compare 2 and 6\n",
    "8. Use RNN to peform sentiment analysis on each verdict,  add sentiment Column for dataset (POSITIVE NETURAL NEGETIVE)\n",
    "9. Dataset Statistics with extra stats:\n",
    "    - number of verdicts each judge gave for every sentiment type\n",
    "    - In Certain TimeFrame: num of verdicts all judges gave for every sentiment type  \n",
    "10. 2-5 subject tags for each verdict using AUTOENCODER  \n",
    "- Compare (3), (4) , (10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- alefbert : https://github.com/OnlpLab/AlephBERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "import typing\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "from gensim import utils\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re\n",
    "import transformers\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "from collections import Counter\n",
    "import json\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec,LineSentence\n",
    "from gensim.corpora import WikiCorpus\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel,pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import fasttext.util\n",
    "import fasttext\n",
    "import time\n",
    "import multiprocessing\n",
    "import spacy\n",
    "import word2vec\n",
    "import tensorflow\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from numpy.random import seed\n",
    "from gensim.corpora import WikiCorpus\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from conlleval import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dirs():\n",
    "    dirs = []\n",
    "    batches = []\n",
    "    for _ ,dirname, files in os.walk(\"./../data\"):\n",
    "        dirs.append(dirname)\n",
    "        f = [file for file in files if file.endswith(\".csv\")]\n",
    "        if len(f) == 0:\n",
    "            continue\n",
    "        f = f[0]\n",
    "        batches.append(f)\n",
    "    queries = dirs[0]\n",
    "    return queries\n",
    "\n",
    "def get_data_path(base_dir,query):\n",
    "    path = os.path.join(base_dir,query,\"csv\",f\"{query}.csv\")\n",
    "    return path\n",
    "\n",
    "def data_gen():\n",
    "    base_dir = \"./../data\"\n",
    "    queries = get_dirs()\n",
    "    for query in queries:\n",
    "        path = get_data_path(base_dir,query)\n",
    "        batch = pd.read_csv(path)\n",
    "        yield batch\n",
    "\n",
    "        \n",
    "queries = get_dirs()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to ./data/nltk...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to ./data/nltk...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to ./data/nltk...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to ./data/nltk...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to ./data/nltk...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to ./data/nltk...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to ./data/nltk...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to ./data/nltk...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to ./data/nltk...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to ./data/nltk...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to ./data/nltk...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to ./data/nltk...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to ./data/nltk...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to ./data/nltk...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to ./data/nltk...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to ./data/nltk...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to ./data/nltk...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to ./data/nltk...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to ./data/nltk...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to ./data/nltk...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to ./data/nltk...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to ./data/nltk...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to ./data/nltk...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to ./data/nltk...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to ./data/nltk...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to ./data/nltk...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to ./data/nltk...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to ./data/nltk...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to ./data/nltk...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to ./data/nltk...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to ./data/nltk...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to ./data/nltk...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to ./data/nltk...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to ./data/nltk...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to ./data/nltk...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to ./data/nltk...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to ./data/nltk...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to ./data/nltk...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to ./data/nltk...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to ./data/nltk...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to ./data/nltk...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to ./data/nltk...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to ./data/nltk...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to ./data/nltk...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to ./data/nltk...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to ./data/nltk...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to ./data/nltk...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to ./data/nltk...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to ./data/nltk...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to ./data/nltk...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to ./data/nltk...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to ./data/nltk...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to ./data/nltk...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to ./data/nltk...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to ./data/nltk...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to ./data/nltk...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to ./data/nltk...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to ./data/nltk...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to ./data/nltk...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to ./data/nltk...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to ./data/nltk...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to ./data/nltk...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to ./data/nltk...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to ./data/nltk...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to ./data/nltk...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to ./data/nltk...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to ./data/nltk...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to ./data/nltk...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to ./data/nltk...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to ./data/nltk...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to ./data/nltk...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to ./data/nltk...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to ./data/nltk...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to ./data/nltk...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to ./data/nltk...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to ./data/nltk...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to ./data/nltk...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to ./data/nltk...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to ./data/nltk...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to ./data/nltk...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to ./data/nltk...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to ./data/nltk...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to ./data/nltk...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to ./data/nltk...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to ./data/nltk...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to ./data/nltk...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     ./data/nltk...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to ./data/nltk...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to ./data/nltk...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to ./data/nltk...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to ./data/nltk...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to ./data/nltk...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to ./data/nltk...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to ./data/nltk...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to ./data/nltk...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to ./data/nltk...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to ./data/nltk...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to ./data/nltk...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to ./data/nltk...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to ./data/nltk...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"all\",download_dir='./data/nltk')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_directory = \"./data/supreme\"\n",
    "file_prefix = \"verdict_batch\"\n",
    "file_type = \"csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILES UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(idx):\n",
    "    filename =  f\"{file_prefix}_{idx}.{file_type}\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verdicts(df:pd.DataFrame):\n",
    "    return df[df['meta_verdict_ty'] == \"פסק-דין\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filesnames():\n",
    "    return [file for file in os.listdir(batch_directory) if file.endswith(f\".{file_type}\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hebrew_words_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        hebrew_words = [line.strip() for line in file]\n",
    "    return hebrew_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_verdicts_data():\n",
    "    if len(get_filesnames()) > 0:\n",
    "        print(True)\n",
    "        return\n",
    "    data = datasets.load_dataset('LevMuchnik/SupremeCourtOfIsrael')\n",
    "    train = data['train']\n",
    "    num_batches = len(train) / BATCH_SIZE\n",
    "    train_loader = train.to_pandas(batch_size=BATCH_SIZE,batched=True)\n",
    "    j = 0\n",
    "    for i , data in tqdm(enumerate(train_loader),total=num_batches):\n",
    "        verdict_df = get_verdicts(data)\n",
    "        if not verdict_df.empty:\n",
    "            file_name = get_filename(j)\n",
    "            verdict_df.to_csv(f\"{batch_directory}/{file_name}\", index=False)\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_file(i,batch,csv_files):\n",
    "    file_path = os.path.join(batch_directory, csv_files[i])\n",
    "    batch.to_csv(file_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORDS UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_hebrew_letter(text:str):\n",
    "    pattern = re.compile(r'[\\u0590-\\u05FF]+')\n",
    "    return bool(re.search(pattern, text))\n",
    "\n",
    "def remove_punctuation_except(text:str, exceptions:list[str]=[]):\n",
    "    punctuation = string.punctuation\n",
    "    for exception in exceptions:\n",
    "        punctuation = punctuation.replace(exception, '')\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_extra_spaces(string):\n",
    "    modified_string = re.sub(r'\\n', ' ', string)\n",
    "    modified_string = re.sub(r'\\s+', ' ', modified_string)\n",
    "    return modified_string\n",
    "\n",
    "def remove_extra_spaces_without_lines(string):\n",
    "    modified_string = re.sub(r'\\n', '@', string)\n",
    "    modified_string = re.sub(r'\\s+', ' ', modified_string)\n",
    "    modified_string = re.sub(r'@', '\\n', modified_string)\n",
    "    return modified_string\n",
    "\n",
    "def is_hebrew_stop_word(word:str,hebrew_stopwords:list[str]):\n",
    "    return word in hebrew_stopwords\n",
    "\n",
    "def valid_non_stop_hebrew_word(text:str,hebrew_stopwords:list[str]):\n",
    "    if len(text) < 2:\n",
    "        return False\n",
    "    if text.isdigit():\n",
    "        return False\n",
    "    if not contains_hebrew_letter(text):\n",
    "        return False\n",
    "    if len(text) == 2 and \"'\" in text:\n",
    "        return False\n",
    "    if is_hebrew_stop_word(text,hebrew_stopwords):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def word_without_prefix(word,prefixes:list[str],word_list:list[str]):\n",
    "    if word in word_list:\n",
    "        return word\n",
    "    for prefix in prefixes:\n",
    "        prefix_len = len(prefix)\n",
    "        if len(word) > prefix_len:\n",
    "            if word[:prefix_len] == prefix:\n",
    "                no_prefix_word = word[prefix_len:]\n",
    "                if no_prefix_word in word_list:\n",
    "                    return no_prefix_word\n",
    "    return word\n",
    "\n",
    "def valid_hebrew_word(text:str):\n",
    "    if len(text) < 2:\n",
    "        return False\n",
    "    if text.isdigit():\n",
    "        return False\n",
    "    if not contains_hebrew_letter(text):\n",
    "        return False\n",
    "    if len(text) == 2 and \"'\" in text:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def str_list_to_list(string_list:str):\n",
    "    lst = string_list.strip(\"[]\")\n",
    "    lst = lst.replace(\"'\", \"\")\n",
    "    lst = lst.split(\",\")\n",
    "    return [item.strip() for item in lst]\n",
    "\n",
    "\n",
    "def super_string_list_to_list(row_item:str):\n",
    "    item =  row_item.strip(\"[]\")\n",
    "    if len(item) == 0:\n",
    "        return []\n",
    "    \n",
    "    item = item.replace(\",\",\" \")\n",
    "    pattern1 = r\"(?<=\\s)'|'(?=\\s)\"\n",
    "    pattern2 = r'(?<=\\s)\"|\"(?=\\s)'\n",
    "    item = re.sub(pattern1, \",\", item)\n",
    "    item = re.sub(pattern2, \",\", item)\n",
    "    if '\"' in item[0]:\n",
    "        item = item[1:]\n",
    "    if \"'\" in item[0]:\n",
    "        item = item[1:]        \n",
    "    l = len(item)\n",
    "    if '\"' in item[l - 1]:\n",
    "        item = item[:l - 1]\n",
    "    l = len(item)\n",
    "    if \"'\" in item[l - 1]:\n",
    "        item = item[:l-1]            \n",
    "    item = remove_extra_spaces(item)\n",
    "    lst = item.split(\",\")\n",
    "    lst = [l.strip() for l in lst if len(l.strip()) > 0]\n",
    "    return lst\n",
    "\n",
    "def reverse_words_order(text:str):\n",
    "    words = text.split()  \n",
    "    reversed_words = list(reversed(words))\n",
    "    reversed_text = ' '.join(reversed_words)\n",
    "    return reversed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_word_dict(d:dict):\n",
    "    sorted_dict = dict(sorted(d.items() ,key=lambda item:item[1],reverse=True))\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keys_from_dict(d:dict,keys:list[str],prefix:int=0):\n",
    "    for prefix in range(prefix):\n",
    "        for rm_word in keys:\n",
    "            if rm_word[prefix:] in d.keys():\n",
    "                del d[rm_word[prefix:]]\n",
    "    \n",
    "            \n",
    "def remove_file_words_from_verdict(idf_dict:dict,path:str,prefix:int=0):\n",
    "    words = read_hebrew_words_file(path)\n",
    "    remove_keys_from_dict(idf_dict,words,prefix)\n",
    "    del words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_bag_of_words(text:str,hebrew_stopwords:list[str],prefixes:list[str],word_list:list[str]):\n",
    "    t = remove_extra_spaces(text).strip()\n",
    "    t = remove_punctuation_except(t,['\"'])\n",
    "    words = t.split(\" \")\n",
    "    bag = Counter(words)\n",
    "    bag = {word_without_prefix(key,prefixes,word_list):value for key,value in bag.items() if valid_non_stop_hebrew_word(key,hebrew_stopwords)}\n",
    "    sorted_bag = dict(sorted(bag.items() ,key=lambda item:item[1],reverse=True))\n",
    "    return sorted_bag\n",
    "\n",
    "def get_bag_of_words(text:str):\n",
    "    t = remove_extra_spaces(text).strip()\n",
    "    t = remove_punctuation_except(t,['\"','-',\"'\"])\n",
    "    words = t.split(\" \")\n",
    "    bag = Counter(words)\n",
    "    bag = {key:value for key,value in bag.items() if valid_hebrew_word(key)}\n",
    "    sorted_bag = dict(sorted(bag.items() ,key=lambda item:item[1],reverse=True))\n",
    "    return sorted_bag\n",
    "\n",
    "def get_unordered_bag_of_words(text:str):\n",
    "    t = remove_extra_spaces(text).strip()\n",
    "    t = remove_punctuation_except(t,['\"','-',\"'\"])\n",
    "    words = t.split(\" \")\n",
    "    words = [word.strip() for word in words if len(word.strip()) > 0]\n",
    "    return words\n",
    "\n",
    "def get_raw_bag_of_words(text:str):\n",
    "    t = remove_extra_spaces(text).strip()\n",
    "    words = t.split(\" \")\n",
    "    words = [word.strip() for word in words if len(word.strip()) > 0]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hebrew_stop_words():\n",
    "    itdk_stopwords = stopwords.words(\"hebrew\")\n",
    "    total_stopwords = []\n",
    "    txt_stopwords = []\n",
    "    with open(\"../heb_stopwords.txt\") as f:\n",
    "        for line in f:\n",
    "            txt_stopwords.append(f.readline())\n",
    "    txt_stopwords = txt_stopwords[:len(txt_stopwords)- 1]\n",
    "    txt_stopwords = [word.split(\"\\n\")[0].strip() for word in txt_stopwords]\n",
    "    total_stopwords.extend(txt_stopwords)\n",
    "    total_stopwords.extend(itdk_stopwords)\n",
    "    total_stopwords = list(set(total_stopwords))\n",
    "    return total_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_path = \"../txt_files/hebrew_wordlists-main/nouns.txt\"\n",
    "adjectives_path = \"../txt_files/hebrew_wordlists-main/adjectives.txt\"\n",
    "places_path = \"../txt_files/hebrew_wordlists-main/israeli_place_names.txt\"\n",
    "prefixes_path = \"../txt_files/hebrew_wordlists-main/prefixes.txt\"\n",
    "preposition_path = \"../txt_files/hebrew_wordlists-main/prepositions.txt\"\n",
    "append_fatverbs_path = \"../txt_files/hebrew_wordlists-main/all_append_fatverb.txt\"\n",
    "before_append_fatverbs_path = \"../txt_files/hebrew_wordlists-main/all_append_fatverb_after_append_some_prefixes.txt\"\n",
    "after_append_fatverbs_path = \"../txt_files/hebrew_wordlists-main/all_append_fatverb_before_append_some_prefixes.txt\"\n",
    "no_fatverbs_path = \"../txt_files/hebrew_wordlists-main/all_no_fatverb.txt\"\n",
    "append_no_fatverbs_prefixes_path = \"../txt_files/hebrew_wordlists-main/all_no_fatverb_append_some_prefixes.txt\"\n",
    "only_no_fatverbs_prefixes_path = \"../txt_files/hebrew_wordlists-main/all_no_fatverb_only_some_prefixes.txt\"\n",
    "with_no_fatverbs_prefixes_path = \"../txt_files/hebrew_wordlists-main/all_no_fatverb_with_some_prefixes.txt\"\n",
    "only_fatverbs_path = \"../txt_files/hebrew_wordlists-main/all_only_fatverb.txt\"\n",
    "append_with_fatverbs_prefixes_path = \"../txt_files/hebrew_wordlists-main/all_with_fatverb_append_some_prefixes.txt\"\n",
    "only_with_fatverbs_prefixes_path = \"../txt_files/hebrew_wordlists-main/all_with_fatverb_only_some_prefixes.txt\"\n",
    "with_with_fatverbs_prefixes_path = \"../txt_files/hebrew_wordlists-main/all_with_fatverb_with_some_prefixes.txt\"\n",
    "with_fatverbs_path = \"../txt_files/hebrew_wordlists-main/all_with_fatverb.txt\"\n",
    "gerunds_path = \"../txt_files/hebrew_wordlists-main/gerunds.txt\"\n",
    "\n",
    "\n",
    "exel_hebrew_stopwords_df = pd.read_excel(\"../stopwords.xlsx\") \n",
    "nltk_txt_hebrew_stopwords_df = read_hebrew_stop_words()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dataset generation using NRE or any other algorithm , deep learning not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "download_verdicts_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = get_filesnames()\n",
    "def batch_generator():\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(batch_directory, csv_file))\n",
    "        yield df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 2-5 subject tags for each verdict using itf-idf or any other algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 it is very complex for hebrew to implememnt:\n",
    "- Lemmatization algorithm \n",
    "- Stemming algorithm   \n",
    "\n",
    "3.2 i could not find a lemmatization or stemming algorithm for hebrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_dict_filename = \"idf_dict\"\n",
    "ROW_TF_IDF = \"tf_idf\"\n",
    "ROW_WORD2VEC = \"word2vec\"\n",
    "ROW_WORD2VEC_SIMILARITY_WORDS= \"word2vec_similarity_words\"\n",
    "ROW_AUTOENCODER = \"autoencoder\"\n",
    "NUM_TAGS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(idf_dict:dict, bag_dict:dict):\n",
    "    words = list(bag_dict.keys())\n",
    "    for word in words:\n",
    "        if not idf_dict.get(word):\n",
    "            idf_dict.update({word:0})\n",
    "    for word,count in bag_dict.items():\n",
    "        idf_dict[word] += count  \n",
    "\n",
    "def normalized_idf(idf_dict:dict):\n",
    "    total = 0\n",
    "    for word,count in idf_dict.items():\n",
    "        total += count\n",
    "    for word,count in idf_dict.items():\n",
    "        idf_dict[word] = count / total\n",
    "\n",
    "\n",
    "def compute_tf(bag_dict:dict): # self freqs = term freq =  tf\n",
    "    total_num_words = 0\n",
    "    for word,count in bag_dict.items():\n",
    "        total_num_words += count\n",
    "    tf_bag = bag_dict.copy()\n",
    "    for word,count in tf_bag.items():\n",
    "        tf_bag[word] = float(count) / float(total_num_words)\n",
    "    return tf_bag\n",
    "\n",
    "def compute_tf_idf(bag_dict:dict,idf_dict:dict):\n",
    "    tf_idf_dict = {}\n",
    "    tf_bag_dict = compute_tf(bag_dict)\n",
    "    for word,count in tf_bag_dict.items():\n",
    "        if word in idf_dict.keys():\n",
    "            tf_idf_dict[word] = count * idf_dict[word]\n",
    "    return tf_idf_dict   \n",
    "\n",
    "def get_idf_dict(generator,files_names,override=True):\n",
    "    if override is False:\n",
    "        if os.path.exists(\"./idf_dict.json\"):\n",
    "            with open('./idf_dict.json') as json_file:\n",
    "                data = json.load(json_file)\n",
    "            return data\n",
    "    idf_dict = {}\n",
    "    for i,batch in tqdm(enumerate(generator()),total=len(files_names)):\n",
    "        batch = batch[batch['text'].notna()]\n",
    "        for index,row in batch.iterrows():\n",
    "            text = row['text']\n",
    "            bag_dict = get_bag_of_words(text)\n",
    "            compute_idf(idf_dict,bag_dict)\n",
    "    return sort_word_dict(idf_dict)\n",
    "\n",
    "\n",
    "\n",
    "def save_dict(d:dict,name:str):\n",
    "    json_data = json.dumps(d,ensure_ascii=False,indent=4)\n",
    "    with open(f'./{name}.json', \"w\",encoding=\"utf-8\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "\n",
    "def tf_idf_df(idf_dict):\n",
    "    for i,batch in tqdm(enumerate(batch_generator()),total=len(csv_files)):\n",
    "        batch[f'{ROW_TF_IDF}_tags'] = ''\n",
    "        batch[ROW_TF_IDF] = ''\n",
    "        for index,row in batch.iterrows():\n",
    "            text = row['text']\n",
    "            bag_dict = get_bag_of_words(text)\n",
    "            d = compute_tf_idf(bag_dict,idf_dict)\n",
    "            d = sort_word_dict(d)\n",
    "            batch.at[index,ROW_TF_IDF] = json.dumps(d)\n",
    "            tags = [word for word, count in d.items()]\n",
    "            t_tags = tags[:NUM_TAGS]\n",
    "            batch.at[index,f'{ROW_TF_IDF}_tags'] = t_tags\n",
    "        update_file(i,batch,csv_files)\n",
    "\n",
    "\n",
    "def get_top_tags_by(row_name:str,num_tags:typing.Optional[int]=None):\n",
    "    all_tags = {}\n",
    "    for i,batch in tqdm(enumerate(batch_generator()),total=len(csv_files)):\n",
    "        if i == 0 :\n",
    "            if row_name not in batch.columns.to_list():\n",
    "                return None\n",
    "        for index,row in batch.iterrows():\n",
    "            tags = str_list_to_list(batch.at[index,f'{row_name}_tags'])\n",
    "            for tag in tags:\n",
    "                if tag not in all_tags.keys():\n",
    "                    all_tags[tag] = 0\n",
    "                if tag in all_tags.keys():\n",
    "                    all_tags[tag] += 1                \n",
    "    all_tags = sort_word_dict(all_tags)\n",
    "    if num_tags is not None:\n",
    "        return list(all_tags.keys())[:num_tags]\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keys_idf(idf_dict:dict):\n",
    "    # lst = list(exel_hebrew_stopwords_df['Undotted'])\n",
    "    # remove_keys_from_dict(idf_dict,lst,2)\n",
    "    return\n",
    "\n",
    "# nouns_path\n",
    "# prefixes_path\n",
    "# stop_words_pathes = [\n",
    "#     adjectives_path,\n",
    "#     places_path,\n",
    "#     prefixes_path,\n",
    "#     preposition_path,\n",
    "#     append_fatverbs_path,\n",
    "#     before_append_fatverbs_path,\n",
    "#     after_append_fatverbs_path,\n",
    "#     no_fatverbs_path,\n",
    "#     append_no_fatverbs_prefixes_path,\n",
    "#     only_no_fatverbs_prefixes_path,\n",
    "#     with_no_fatverbs_prefixes_path,\n",
    "#     only_fatverbs_path,\n",
    "#     append_with_fatverbs_prefixes_path,\n",
    "#     only_with_fatverbs_prefixes_path,\n",
    "#     with_with_fatverbs_prefixes_path,\n",
    "#     with_fatverbs_path,\n",
    "#     gerunds_path\n",
    "# ]\n",
    "# exel_hebrew_stopwords_df\n",
    "# nltk_txt_hebrew_stopwords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346601\n"
     ]
    }
   ],
   "source": [
    "idf_dict = get_idf_dict(batch_generator,csv_files,override=False)\n",
    "print(len(idf_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346601\n"
     ]
    }
   ],
   "source": [
    "remove_keys_idf(idf_dict)\n",
    "print(len(idf_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict(idf_dict,idf_dict_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case_id',\n",
       " 'download_time',\n",
       " 'number_of_case_documents',\n",
       " 'file_name',\n",
       " 'Id',\n",
       " 'CaseId',\n",
       " 'VerdictDt',\n",
       " 'CreatedDate',\n",
       " 'CaseNum',\n",
       " 'CaseDesc',\n",
       " 'Pages',\n",
       " 'Path',\n",
       " 'CaseName',\n",
       " 'FileName',\n",
       " 'DocName',\n",
       " 'Year',\n",
       " 'TypeCode',\n",
       " 'Type',\n",
       " 'Technical',\n",
       " 'CodeVolume',\n",
       " 'document_hash',\n",
       " 'text',\n",
       " 'html_title',\n",
       " 'VerdictsDt',\n",
       " 'meta_case_nm',\n",
       " 'meta_sec_appeal',\n",
       " 'meta_side_ty',\n",
       " 'meta_verdict_file_nm',\n",
       " 'meta_judge',\n",
       " 'meta_mador_nm',\n",
       " 'meta_side_nm',\n",
       " 'meta_verdict_dt',\n",
       " 'meta_case_dt',\n",
       " 'meta_verdict_nbr',\n",
       " 'meta_ProgId',\n",
       " 'meta_is_technical',\n",
       " 'meta_judge_nm_last',\n",
       " 'meta_case_nbr',\n",
       " 'meta_verdict_ty',\n",
       " 'meta_lawyer_nm',\n",
       " 'meta_judge_nm_first',\n",
       " 'meta_verdict_pages',\n",
       " 'meta_inyan_nm',\n",
       " 'meta_court_nm',\n",
       " 'sentiment',\n",
       " 'tags',\n",
       " 'tf_idf',\n",
       " 'word2vec_tags',\n",
       " 'word2vec',\n",
       " 'word2vec_similarity_words',\n",
       " 'tf_idf_tags']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_verdict_batch = pd.read_csv('./data/supreme/verdict_batch_0.csv')\n",
    "verdict_columns = example_verdict_batch.columns.to_list()\n",
    "verdict_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df(idf_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 2-5 subject tags for each verdict using Word2Vec (same as 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_path = \"./models/verdicts_word2vec.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerdictsCorpus:\n",
    "    def __init__(self,csv_files) -> None:\n",
    "        self.csv_files = csv_files\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i,batch in tqdm(enumerate(batch_generator()),total=len(csv_files)):\n",
    "            for index,row in batch.iterrows():\n",
    "                text = row['text']\n",
    "                yield utils.simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_model(path,train=False):\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Creating Model\")\n",
    "        sentences = VerdictsCorpus(get_filesnames())\n",
    "        model = Word2Vec(sentences=sentences)\n",
    "        model.save(path)\n",
    "        if train is True:\n",
    "            print(\"Training Model\")\n",
    "    print(\"Loading Model\")\n",
    "    model =  Word2Vec.load(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model\n"
     ]
    }
   ],
   "source": [
    "model = word2vec_model(word2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:31<00:00, 11.02it/s]\n",
      "100%|██████████| 352/352 [00:41<00:00,  8.41it/s]\n",
      "100%|██████████| 352/352 [00:41<00:00,  8.47it/s]\n",
      "100%|██████████| 352/352 [00:40<00:00,  8.79it/s]\n",
      "100%|██████████| 352/352 [00:45<00:00,  7.71it/s]\n",
      "100%|██████████| 352/352 [00:42<00:00,  8.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity_scores_to_similarity_words_averages_dict(similarity_scores:dict):\n",
    "    sim_words = {}\n",
    "    for word,sim in similarity_scores.items():\n",
    "        for sim_score in sim:\n",
    "            sim_word = sim_score[0]\n",
    "            sim_score = sim_score[1]\n",
    "\n",
    "            if sim_word not in sim_words.keys():\n",
    "                sim_words[sim_word] = []\n",
    "            if sim_word in sim_words.keys():\n",
    "                sim_words[sim_word].append(sim_score)\n",
    "\n",
    "    for sim_word, scores in sim_words.items():\n",
    "        total = 0\n",
    "        for score in scores:\n",
    "            total += score\n",
    "        score = float(total) / float(len(scores))\n",
    "        sim_words[sim_word] = score\n",
    "\n",
    "    return sim_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_df(generator,filenames,override=False):        \n",
    "    for i,batch in tqdm_notebook(enumerate(generator()),total=len(filenames),desc=\"Batches\",position=0):\n",
    "        if not ROW_WORD2VEC in verdict_columns:\n",
    "            batch[f'{ROW_WORD2VEC}_tags'] = ''\n",
    "            batch[ROW_WORD2VEC] = ''\n",
    "            batch[ROW_WORD2VEC_SIMILARITY_WORDS] = ''\n",
    "        else:\n",
    "            if override is False:\n",
    "                print(\"Data Exists - Not Over Writing\")\n",
    "                return\n",
    "        for j,(index,row) in tqdm_notebook(enumerate(batch.iterrows()),desc=\"Batch\",leave=False,total=len(batch),position=1):\n",
    "            text = row['text']\n",
    "            preprocessed_text = utils.simple_preprocess(text)\n",
    "            word_vectors = {word:model.wv[word] for word in preprocessed_text if word in model.wv.key_to_index}\n",
    "            similarity_scores = {}\n",
    "            for j,(word,vector) in enumerate(word_vectors.items()):\n",
    "                sim = model.wv.similar_by_vector(vector,topn=10)\n",
    "                similarity_scores[word] = sorted(sim,key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            sim_words = word_similarity_scores_to_similarity_words_averages_dict(similarity_scores)\n",
    "            sim_words = sort_word_dict(sim_words)\n",
    "            tags = list(sim_words.keys())\n",
    "            t_tags = tags[:NUM_TAGS]\n",
    "            batch.at[index,ROW_WORD2VEC_SIMILARITY_WORDS] = json.dumps(sim_words)\n",
    "            batch.at[index,ROW_WORD2VEC] = json.dumps(similarity_scores)\n",
    "            batch.at[index,f'{ROW_WORD2VEC}_tags'] = t_tags\n",
    "        update_file(i,batch,filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107507/1397935610.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i,batch in tqdm_notebook(enumerate(batch_generator()),total=len(csv_files),desc=\"Batches\",position=0):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c438ad7b3840e999cb5f19c2b1430d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Exists - Not Over Writing\n"
     ]
    }
   ],
   "source": [
    "word2vec_df(batch_generator,csv_files,override=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare (3) and (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tags(col1,col2):\n",
    "    bins = []\n",
    "    for t in range(NUM_TAGS):\n",
    "        b = t\n",
    "        bins.append(b)\n",
    "\n",
    "    bins_dict = {f\" similar-{b} \":0 for b in bins}\n",
    "    bins_names = list(bins_dict.keys())\n",
    "    for i,batch in tqdm(enumerate(batch_generator()),total=len(csv_files),desc=\"Batches\"):\n",
    "        col1_name = f'{col1}_tags'\n",
    "        col2_name = f'{col2}_tags'\n",
    "        for index,row in batch.iterrows():\n",
    "            tags1 = str_list_to_list(batch.at[index,col1_name])\n",
    "            tags2 = str_list_to_list(batch.at[index,col2_name])\n",
    "            tags = list(set(tags1 + tags2))\n",
    "            similar = 0\n",
    "            for tag in tags:\n",
    "                if tag in tags1 and tag in tags2:\n",
    "                    similar += 1\n",
    "            for j,bin in enumerate(bins):\n",
    "                if similar == bin[0]:\n",
    "                    bin_name = bins_names[j]\n",
    "                    bins_dict[bin_name] += 1\n",
    "    return bins_dict\n",
    "\n",
    "def plot_bins(bins_dict:dict):\n",
    "    bin_labels = list(bins_dict.keys())\n",
    "    bin_counts = list(bins_dict.values())\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(bin_labels, bin_counts)\n",
    "    ax.set_xlabel(\"Num Similar Tags\") \n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Tags Similary Comparison\")  # Plot title\n",
    "    plt.show()\n",
    "\n",
    "def compare_top_tags(lst1,lst1_name,lst2,lst2_name,num_tags):\n",
    "    print(\"tags: \",lst1_name,\"|\",lst2_name)\n",
    "    for i in range(num_tags):\n",
    "        tag1 = lst1[i]\n",
    "        tag2 = lst2[i]\n",
    "        print(f\"{i}. \",tag1,\"|\",tag2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 352/352 [01:21<00:00,  4.34it/s]\n"
     ]
    }
   ],
   "source": [
    "bins_dict = compare_tags(ROW_WORD2VEC,ROW_TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_bins(bins_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [01:19<00:00,  4.45it/s]\n",
      "100%|██████████| 352/352 [01:23<00:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags:  tf_idf | word2vec\n",
      "0.  על | לצדק\n",
      "1.  של | ישראל\n",
      "2.  המשפט | פסק\n",
      "3.  את | הישיבה\n",
      "4.  כבוד | בע\n",
      "5.  כי | ברק\n",
      "6.  בית | המשיבים\n",
      "7.  המערער | בת\n",
      "8.  לא | צו\n",
      "9.  עו\"ד | ללא\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_COMPARISON_TAGS = 10\n",
    "top_10_tags_tf_idf = get_top_tags_by(ROW_TF_IDF,num_tags=NUM_COMPARISON_TAGS)\n",
    "top_10_tags_word2vec = get_top_tags_by(ROW_WORD2VEC,num_tags=NUM_COMPARISON_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags:  tf_idf | word2vec\n",
      "0.  על | לצדק\n",
      "1.  של | ישראל\n",
      "2.  המשפט | פסק\n",
      "3.  את | הישיבה\n",
      "4.  כבוד | בע\n",
      "5.  כי | ברק\n",
      "6.  בית | המשיבים\n",
      "7.  המערער | בת\n",
      "8.  לא | צו\n",
      "9.  עו\"ד | ללא\n"
     ]
    }
   ],
   "source": [
    "compare_top_tags(top_10_tags_tf_idf,\"tf_idf\",top_10_tags_word2vec,\"word2vec\",NUM_COMPARISON_TAGS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Statistics\n",
    "    - In Certain TimeFrame:\n",
    "    - a. Number of verdict of each judge\n",
    "    - b. Number of verdicts each judge gave in the top 10 popular subject tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2004-01-01\"\n",
    "end_date = \"2005-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_judge_names(first_names,last_names):\n",
    "    first = first_names.strip('[ ]')\n",
    "    first = first.replace(\"'\", \"\")\n",
    "    first = first.split(' ')\n",
    "    last = last_names.strip('[ ]')\n",
    "    last = last.replace(\"'\", \"\")\n",
    "    last = last.replace('\"', \"\")\n",
    "    last = last.split(' ')\n",
    "    judge_names = [first[i] + \" \" + last[i] for i in range(len(first))]\n",
    "    return judge_names\n",
    "\n",
    "\n",
    "def get_date_from_string(dt):\n",
    "    return datetime.datetime.strptime(dt,\"%Y-%m-%d\")\n",
    "\n",
    "def is_in_date_range(start_date,end_date,date):\n",
    "    start_date = get_date_from_string(start_date)\n",
    "    end_date = get_date_from_string(end_date)\n",
    "    date = get_date_from_string(date)\n",
    "    if start_date <= date <= end_date:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_v1(start_date,end_date,top_10_tags,col_name):\n",
    "    top_tags_counts = {tag:0 for tag in top_10_tags}\n",
    "    judges_num_verdicts = { }\n",
    "    for i,batch in tqdm(enumerate(batch_generator()),total=len(csv_files)):\n",
    "        for index,row in batch.iterrows():\n",
    "            text = row['text']\n",
    "            verdict_date = row['meta_verdict_dt']\n",
    "            first = row['meta_judge_nm_first']\n",
    "            last = row['meta_judge_nm_last']\n",
    "            tags = str_list_to_list(row[f'{col_name}_tags'])\n",
    "            is_in_range = is_in_date_range(start_date,end_date,verdict_date)\n",
    "            if not is_in_range:\n",
    "                continue\n",
    "            judge_names = get_judge_names(first,last)\n",
    "            for judge in judge_names:\n",
    "                if judge not in judges_num_verdicts.keys():\n",
    "                    judges_num_verdicts[judge] = 0\n",
    "                judges_num_verdicts[judge] += 1\n",
    "\n",
    "            for tag in tags:\n",
    "                if tag in top_tags_counts.keys():\n",
    "                    top_tags_counts[tag] += 1\n",
    "                    \n",
    "    return top_tags_counts,judges_num_verdicts\n",
    "\n",
    "def print_stats_v1(title,start_date,end_date,top_tags_counts,judges_num_verdicts):\n",
    "    print(f\"-- {title} --\")\n",
    "    print('start date: ',start_date, \"end date: \", end_date)\n",
    "    print(\"in this timeframe:\")\n",
    "    print()\n",
    "    print(\"top 10 tags:\")\n",
    "    for tag,count in top_tags_counts.items():\n",
    "        print(f\"{tag}:\",count)\n",
    "    print()\n",
    "    print(\"judges number of verdicts:\")\n",
    "    for judge,num_vertics in judges_num_verdicts.items():\n",
    "        print(f\"{judge}:\",num_vertics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [01:14<00:00,  4.72it/s]\n"
     ]
    }
   ],
   "source": [
    "top_tags_counts1,judges_num_verdicts1 = get_stats_v1(start_date,end_date,top_10_tags_tf_idf,ROW_TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- tf_idf tags --\n",
      "start date:  2004-01-01 end date:  2005-01-01\n",
      "in this timeframe:\n",
      "\n",
      "top 10 tags:\n",
      "על: 2287\n",
      "של: 2226\n",
      "המשפט: 1504\n",
      "את: 1714\n",
      "כבוד: 1359\n",
      "כי: 1488\n",
      "בית: 443\n",
      "המערער: 612\n",
      "לא: 424\n",
      "עו\"ד: 365\n",
      "\n",
      "judges number of verdicts:\n",
      "דורית ביניש: 721\n",
      "אילה פרוקציה: 512\n",
      "סלים גובראן: 515\n",
      "אהרן ברק: 636\n",
      "תאודור אור: 110\n",
      "אליהו מצא: 462\n",
      "אדמונד לוי: 637\n",
      "אשר גרוניס: 667\n",
      "מרים נאור: 760\n",
      "עדנה ארבל: 112\n",
      "מישאל חשין: 425\n",
      "אליעזר ריבלין: 570\n",
      "אליקים רובינשטיין: 129\n",
      "דליה דורנר: 137\n",
      "יעקב טירקל: 233\n",
      "אסתר חיות: 562\n",
      "חגית מאק-קלמנוביץ: 115\n",
      "אילה\n",
      " פרוקציה\n",
      ": 3\n",
      "יהונתן עדיאל: 332\n",
      "יגאל מרזל: 3\n",
      "טובה שטרסברג-כהן: 1\n",
      "עודד שחם: 5\n",
      "בעז אוקון: 2\n",
      "שלמה לוין: 1\n"
     ]
    }
   ],
   "source": [
    "print_stats_v1(\"tf_idf tags\",start_date,end_date,top_tags_counts1,judges_num_verdicts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [01:29<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "top_tags_counts2,judges_num_verdicts2 = get_stats_v1(start_date,end_date,top_10_tags_word2vec,ROW_WORD2VEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- word2vec tags --\n",
      "start date:  2004-01-01 end date:  2005-01-01\n",
      "in this timeframe:\n",
      "\n",
      "top 10 tags:\n",
      "לצדק: 1067\n",
      "ישראל: 857\n",
      "פסק: 747\n",
      "הישיבה: 692\n",
      "בע: 633\n",
      "ברק: 615\n",
      "המשיבים: 395\n",
      "בת: 379\n",
      "צו: 342\n",
      "ללא: 388\n",
      "\n",
      "judges number of verdicts:\n",
      "דורית ביניש: 721\n",
      "אילה פרוקציה: 512\n",
      "סלים גובראן: 515\n",
      "אהרן ברק: 636\n",
      "תאודור אור: 110\n",
      "אליהו מצא: 462\n",
      "אדמונד לוי: 637\n",
      "אשר גרוניס: 667\n",
      "מרים נאור: 760\n",
      "עדנה ארבל: 112\n",
      "מישאל חשין: 425\n",
      "אליעזר ריבלין: 570\n",
      "אליקים רובינשטיין: 129\n",
      "דליה דורנר: 137\n",
      "יעקב טירקל: 233\n",
      "אסתר חיות: 562\n",
      "חגית מאק-קלמנוביץ: 115\n",
      "אילה\n",
      " פרוקציה\n",
      ": 3\n",
      "יהונתן עדיאל: 332\n",
      "יגאל מרזל: 3\n",
      "טובה שטרסברג-כהן: 1\n",
      "עודד שחם: 5\n",
      "בעז אוקון: 2\n",
      "שלמה לוין: 1\n"
     ]
    }
   ],
   "source": [
    "print_stats_v1(\"word2vec tags\",start_date,end_date,top_tags_counts2,judges_num_verdicts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use RNN to perform NER recognision for dataset generation(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:12<00:00,  2.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'של': 143268,\n",
       " 'כי': 98162,\n",
       " 'על': 96816,\n",
       " 'את': 89038,\n",
       " 'לא': 45800,\n",
       " 'בית': 36221,\n",
       " 'המערער': 34541,\n",
       " 'המשפט': 34374,\n",
       " 'זה': 33403,\n",
       " 'או': 25872,\n",
       " 'גם': 24133,\n",
       " 'זו': 21111,\n",
       " 'הוא': 20374,\n",
       " 'אשר': 20266,\n",
       " 'אין': 19404,\n",
       " 'בין': 19062,\n",
       " 'הדין': 18798,\n",
       " 'כך': 17641,\n",
       " 'היא': 17254,\n",
       " 'יש': 16226,\n",
       " 'עם': 16081,\n",
       " 'סעיף': 16039,\n",
       " 'אם': 15941,\n",
       " 'להלן': 15802,\n",
       " 'לחוק': 15101,\n",
       " 'זאת': 14972,\n",
       " 'כל': 14919,\n",
       " 'כן': 13799,\n",
       " 'היה': 13269,\n",
       " 'השופט': 12965,\n",
       " 'פסקה': 12948,\n",
       " 'ניתן': 12940,\n",
       " 'המחוזי': 12582,\n",
       " 'משפט': 12576,\n",
       " 'ראו': 12368,\n",
       " 'אף': 12140,\n",
       " 'לאחר': 11576,\n",
       " 'עניין': 11260,\n",
       " 'ישראל': 10759,\n",
       " 'המשיב': 10306,\n",
       " 'באופן': 10170,\n",
       " 'כדי': 10077,\n",
       " 'ידי': 9976,\n",
       " 'שלא': 9394,\n",
       " 'בו': 9148,\n",
       " 'בעניין': 9021,\n",
       " 'לפי': 8915,\n",
       " 'אינו': 8583,\n",
       " 'לו': 8582,\n",
       " 'כאמור': 8520,\n",
       " 'ביחס': 8439,\n",
       " 'אינה': 8401,\n",
       " 'אלה': 8222,\n",
       " 'לכך': 8157,\n",
       " 'במסגרת': 8138,\n",
       " 'כאשר': 8016,\n",
       " 'כפי': 7973,\n",
       " 'מדובר': 7846,\n",
       " 'עוד': 7809,\n",
       " 'ביום': 7776,\n",
       " 'אלא': 7773,\n",
       " 'ולא': 7624,\n",
       " 'דין': 7494,\n",
       " 'מקום': 7239,\n",
       " 'הצדדים': 7138,\n",
       " 'דינו': 7114,\n",
       " 'פסק': 7087,\n",
       " 'אך': 7048,\n",
       " 'חוק': 6974,\n",
       " 'קמא': 6814,\n",
       " 'המשיבים': 6653,\n",
       " 'נקבע': 6610,\n",
       " 'הערעור': 6569,\n",
       " 'המדינה': 6527,\n",
       " 'בנסיבות': 6482,\n",
       " 'לבין': 6434,\n",
       " 'בכך': 6165,\n",
       " 'הן': 6116,\n",
       " 'בהתאם': 6087,\n",
       " 'מן': 6085,\n",
       " 'מ\"בע': 6043,\n",
       " 'רק': 5999,\n",
       " 'בה': 5996,\n",
       " 'וכי': 5940,\n",
       " 'נטען': 5796,\n",
       " 'המערערת': 5759,\n",
       " 'אחד': 5653,\n",
       " 'האם': 5623,\n",
       " 'המשיבה': 5553,\n",
       " 'כלל': 5550,\n",
       " \"מדינת'\": 5510,\n",
       " 'שבו': 5501,\n",
       " 'מיום': 5468,\n",
       " 'אלו': 5432,\n",
       " 'לעיל': 5381,\n",
       " 'הדברים': 5359,\n",
       " 'צו': 5354,\n",
       " 'פ\"ע': 5250,\n",
       " 'שם': 5233,\n",
       " 'בבית': 5222,\n",
       " 'א\"ע': 5188,\n",
       " 'ד\"פ': 5175,\n",
       " 'בסעיף': 5116,\n",
       " 'חברי': 5088,\n",
       " 'בגין': 5070,\n",
       " 'ד\"עו': 5035,\n",
       " 'עד': 4973,\n",
       " 'הם': 4953,\n",
       " 'כמו': 4944,\n",
       " 'בהקשר': 4906,\n",
       " 'אני': 4885,\n",
       " 'במקרה': 4747,\n",
       " 'פי': 4697,\n",
       " 'לצורך': 4687,\n",
       " 'האמור': 4679,\n",
       " 'יותר': 4648,\n",
       " 'דבר': 4645,\n",
       " 'בכל': 4642,\n",
       " 'אותו': 4633,\n",
       " 'היתר': 4615,\n",
       " 'וכן': 4614,\n",
       " 'לפני': 4610,\n",
       " 'אדם': 4606,\n",
       " 'מס': 4572,\n",
       " 'העבירה': 4545,\n",
       " 'אחר': 4490,\n",
       " 'בדבר': 4479,\n",
       " 'החברה': 4463,\n",
       " 'ועל': 4404,\n",
       " 'בשל': 4401,\n",
       " 'תוך': 4305,\n",
       " 'לעניין': 4294,\n",
       " 'מאסר': 4272,\n",
       " 'לפסק': 4267,\n",
       " 'שכן': 4261,\n",
       " 'הדיון': 4259,\n",
       " 'דנן': 4251,\n",
       " 'לבית': 4197,\n",
       " 'עולה': 4180,\n",
       " 'קבע': 4167,\n",
       " 'טענות': 4149,\n",
       " 'הייתה': 4140,\n",
       " 'כבוד': 4132,\n",
       " 'החוק': 4126,\n",
       " 'הביטוח': 4111,\n",
       " 'בפסק': 4063,\n",
       " 'הוצאות': 3998,\n",
       " 'לה': 3957,\n",
       " 'המנוח': 3948,\n",
       " 'ההליך': 3923,\n",
       " 'דעתו': 3903,\n",
       " 'העותר': 3883,\n",
       " 'ערעור': 3832,\n",
       " 'השופטת': 3829,\n",
       " 'המערערים': 3747,\n",
       " 'לפיה': 3737,\n",
       " 'למשל': 3723,\n",
       " 'באשר': 3719,\n",
       " 'התביעה': 3717,\n",
       " 'בהם': 3661,\n",
       " 'בהליך': 3643,\n",
       " 'שנים': 3617,\n",
       " 'שלפנינו': 3582,\n",
       " 'ככל': 3531,\n",
       " 'היו': 3528,\n",
       " 'עליו': 3507,\n",
       " 'טענה': 3486,\n",
       " 'בפסקה': 3467,\n",
       " 'בפועל': 3462,\n",
       " 'שניתן': 3461,\n",
       " 'וזאת': 3459,\n",
       " 'אחת': 3453,\n",
       " \"'בעמ\": 3452,\n",
       " 'בלבד': 3451,\n",
       " 'הליך': 3446,\n",
       " 'נגד': 3439,\n",
       " 'ץ\"בג': 3422,\n",
       " 'המנוחה': 3409,\n",
       " 'המתלוננת': 3408,\n",
       " 'שיש': 3387,\n",
       " 'הדעת': 3384,\n",
       " 'חלק': 3364,\n",
       " 'מספר': 3354,\n",
       " 'אותה': 3331,\n",
       " 'אולם': 3319,\n",
       " 'לקבל': 3312,\n",
       " 'העתירה': 3311,\n",
       " 'ביצוע': 3310,\n",
       " 'הראיות': 3279,\n",
       " 'ללא': 3277,\n",
       " 'אכן': 3270,\n",
       " 'בענייננו': 3266,\n",
       " 'בשם': 3265,\n",
       " 'דעת': 3262,\n",
       " 'ואף': 3253,\n",
       " 'אל': 3250,\n",
       " 'מכוח': 3244,\n",
       " 'באמצעות': 3242,\n",
       " 'כלפי': 3240,\n",
       " 'הרצח': 3231,\n",
       " 'נוסף': 3203,\n",
       " 'רשות': 3177,\n",
       " 'שהוא': 3170,\n",
       " 'הסדר': 3168,\n",
       " 'לקבוע': 3153,\n",
       " 'זכות': 3138,\n",
       " 'אפוא': 3133,\n",
       " 'ידי-על': 3116,\n",
       " 'ניהול': 3105,\n",
       " 'להיות': 3079,\n",
       " 'בשלב': 3068,\n",
       " 'במהלך': 3014,\n",
       " 'דיני': 2991,\n",
       " 'לגבי': 2991,\n",
       " 'עונש': 2981,\n",
       " 'האישום': 2980,\n",
       " 'חוות': 2973,\n",
       " 'מה': 2964,\n",
       " 'שאין': 2951,\n",
       " 'בני': 2946,\n",
       " 'מדינת': 2942,\n",
       " 'הוראות': 2917,\n",
       " 'העובדה': 2909,\n",
       " 'מנת': 2890,\n",
       " 'מי': 2876,\n",
       " 'מתן': 2858,\n",
       " 'הנאשם': 2849,\n",
       " 'ח\"ש': 2839,\n",
       " 'לכל': 2838,\n",
       " 'המשפטי': 2806,\n",
       " 'במקרים': 2795,\n",
       " 'בלתי': 2787,\n",
       " 'בעת': 2782,\n",
       " 'לצד': 2772,\n",
       " 'לב': 2748,\n",
       " 'אינם': 2745,\n",
       " 'מאחר': 2744,\n",
       " 'שימוש': 2729,\n",
       " 'פלוני': 2715,\n",
       " 'מעשה': 2710,\n",
       " 'המס': 2709,\n",
       " 'שני': 2702,\n",
       " 'טענת': 2698,\n",
       " 'בעל': 2675,\n",
       " 'עצמו': 2667,\n",
       " '\"כי': 2665,\n",
       " 'דברים': 2658,\n",
       " 'הראשון': 2655,\n",
       " 'חברות': 2648,\n",
       " 'עבירות': 2647,\n",
       " 'הרי': 2647,\n",
       " 'נוכח': 2643,\n",
       " 'בפני': 2626,\n",
       " 'המבקשים': 2625,\n",
       " 'אחרת': 2622,\n",
       " 'בן': 2610,\n",
       " 'סביר': 2604,\n",
       " 'לחוות': 2601,\n",
       " 'להם': 2600,\n",
       " 'בישראל': 2600,\n",
       " 'קשר': 2589,\n",
       " 'כבר': 2575,\n",
       " 'בידי': 2565,\n",
       " 'רקע': 2560,\n",
       " 'בקשה': 2558,\n",
       " 'יכול': 2554,\n",
       " 'תנאי': 2548,\n",
       " 'בהמשך': 2543,\n",
       " 'יהיה': 2516,\n",
       " 'בחוק': 2506,\n",
       " 'העונש': 2504,\n",
       " 'קבלת': 2503,\n",
       " 'השני': 2496,\n",
       " 'לבחון': 2456,\n",
       " 'החלטה': 2455,\n",
       " 'תקנה': 2446,\n",
       " 'כתב': 2445,\n",
       " 'עבירת': 2441,\n",
       " 'המקרה': 2438,\n",
       " 'לשון': 2431,\n",
       " 'ואולם': 2410,\n",
       " 'אחרים': 2407,\n",
       " 'שיקול': 2389,\n",
       " 'פסקאות': 2387,\n",
       " 'משום': 2387,\n",
       " 'עבירה': 2386,\n",
       " 'לטענת': 2368,\n",
       " 'ראיות': 2366,\n",
       " 'מכך': 2358,\n",
       " 'משקל': 2357,\n",
       " 'לתקנות': 2351,\n",
       " \"'עמ\": 2344,\n",
       " 'השאלה': 2342,\n",
       " 'בקשת': 2336,\n",
       " 'נדרש': 2323,\n",
       " 'האירוע': 2323,\n",
       " 'מהווה': 2318,\n",
       " 'שבהם': 2311,\n",
       " 'הוועדה': 2309,\n",
       " 'בהן': 2308,\n",
       " 'צוין': 2303,\n",
       " 'נסיבות': 2298,\n",
       " 'המחוקק': 2296,\n",
       " 'העניין': 2293,\n",
       " 'מבלי': 2282,\n",
       " 'בתי': 2280,\n",
       " 'מידע': 2278,\n",
       " 'דיון': 2273,\n",
       " 'סבור': 2264,\n",
       " 'הסכם': 2263,\n",
       " 'הכנסה': 2262,\n",
       " 'הרשות': 2261,\n",
       " 'יסוד': 2260,\n",
       " 'שלו': 2251,\n",
       " 'העונשין': 2234,\n",
       " 'הנוגע': 2234,\n",
       " 'למעשה': 2230,\n",
       " 'בסיס': 2226,\n",
       " 'העותרים': 2222,\n",
       " 'שונים': 2220,\n",
       " 'ועדת': 2219,\n",
       " 'הדבר': 2218,\n",
       " 'שהיא': 2217,\n",
       " 'תביעה': 2208,\n",
       " 'אחריות': 2205,\n",
       " 'לשם': 2190,\n",
       " 'הזוג': 2179,\n",
       " 'מטעם': 2176,\n",
       " 'די': 2171,\n",
       " 'ספק': 2164,\n",
       " 'לעשות': 2138,\n",
       " 'ההורות': 2136,\n",
       " 'דומה': 2134,\n",
       " 'הציבור': 2132,\n",
       " 'קיים': 2125,\n",
       " 'תקופת': 2113,\n",
       " 'האפשרות': 2103,\n",
       " 'דמי': 2102,\n",
       " 'חברת': 2086,\n",
       " 'המשנה': 2079,\n",
       " 'בעקבות': 2077,\n",
       " 'החלטת': 2074,\n",
       " 'בכתב': 2065,\n",
       " 'זמן': 2063,\n",
       " 'שירות': 2052,\n",
       " 'מקרקעין': 2044,\n",
       " 'ציין': 2035,\n",
       " 'תיקון': 2034,\n",
       " 'לראות': 2031,\n",
       " 'זכויות': 2028,\n",
       " 'ההחלטה': 2016,\n",
       " 'דרך': 1997,\n",
       " 'למתן': 1996,\n",
       " 'הבקשה': 1990,\n",
       " 'שונות': 1988,\n",
       " 'עסקינן': 1988,\n",
       " 'העותרת': 1980,\n",
       " 'נוספת': 1974,\n",
       " 'לרבות': 1972,\n",
       " 'בניגוד': 1964,\n",
       " 'מעבר': 1957,\n",
       " 'לנוכח': 1950,\n",
       " 'בשנת': 1945,\n",
       " 'בעניינו': 1943,\n",
       " 'בנוגע': 1939,\n",
       " 'שלפיה': 1935,\n",
       " 'בתוך': 1929,\n",
       " 'מועד': 1929,\n",
       " 'אינן': 1920,\n",
       " 'מקרה': 1919,\n",
       " 'לאור': 1918,\n",
       " 'היום': 1915,\n",
       " 'למסקנה': 1915,\n",
       " 'בהינתן': 1899,\n",
       " 'המבקש': 1899,\n",
       " 'ההשקעות': 1894,\n",
       " 'פרשנות': 1865,\n",
       " 'שמדובר': 1864,\n",
       " 'שבה': 1860,\n",
       " 'נוספים': 1859,\n",
       " 'הורות': 1856,\n",
       " 'כדין': 1854,\n",
       " 'הגשת': 1848,\n",
       " 'ואין': 1846,\n",
       " 'לדין': 1838,\n",
       " 'בדרך': 1836,\n",
       " 'ההתיישנות': 1836,\n",
       " 'נעשה': 1834,\n",
       " 'להורות': 1831,\n",
       " 'בעלי': 1829,\n",
       " 'שנקבע': 1825,\n",
       " 'ההגנה': 1824,\n",
       " 'פגיעה': 1822,\n",
       " 'למערער': 1817,\n",
       " 'ואת': 1815,\n",
       " 'העליון': 1808,\n",
       " 'הוראת': 1806,\n",
       " 'הפלילי': 1806,\n",
       " 'שהמערער': 1798,\n",
       " 'רשאי': 1796,\n",
       " 'בגדר': 1795,\n",
       " 'מתוך': 1794,\n",
       " 'ממנו': 1792,\n",
       " 'להביא': 1790,\n",
       " 'ובין': 1786,\n",
       " 'והן': 1784,\n",
       " 'שאינו': 1783,\n",
       " 'פני': 1782,\n",
       " 'אמת': 1781,\n",
       " 'ועוד': 1780,\n",
       " 'לפנינו': 1775,\n",
       " 'במצב': 1772,\n",
       " 'אותם': 1769,\n",
       " 'ראוי': 1767,\n",
       " 'השימוש': 1765,\n",
       " 'בעבירות': 1763,\n",
       " 'ערך': 1762,\n",
       " 'חברה': 1760,\n",
       " 'החילוט': 1760,\n",
       " 'ההסדר': 1750,\n",
       " 'עמדה': 1749,\n",
       " 'מבחן': 1749,\n",
       " '\"של': 1749,\n",
       " 'הטענה': 1739,\n",
       " 'באותו': 1737,\n",
       " 'לפיכך': 1729,\n",
       " 'היתה': 1728,\n",
       " 'יום': 1724,\n",
       " 'ראשית': 1722,\n",
       " 'לדחות': 1718,\n",
       " 'במישור': 1717,\n",
       " 'בנוסף': 1712,\n",
       " 'לאפשר': 1709,\n",
       " 'מכל': 1708,\n",
       " 'סמכות': 1708,\n",
       " 'עמד': 1699,\n",
       " 'לממשלה': 1690,\n",
       " 'ביותר': 1682,\n",
       " 'שם\"': 1676,\n",
       " 'רצח': 1663,\n",
       " 'להגיש': 1659,\n",
       " 'תחילה': 1658,\n",
       " 'עבור': 1655,\n",
       " 'אפשרות': 1655,\n",
       " 'הראשונה': 1653,\n",
       " 'חובה': 1651,\n",
       " 'לטובת': 1643,\n",
       " 'הנזק': 1642,\n",
       " 'הסף': 1641,\n",
       " 'להידחות': 1639,\n",
       " 'הרע': 1639,\n",
       " 'ממש': 1637,\n",
       " 'המותב': 1631,\n",
       " 'במשפט': 1619,\n",
       " 'ההוצאות': 1614,\n",
       " 'שבין': 1608,\n",
       " 'המועד': 1603,\n",
       " 'הפגיעה': 1600,\n",
       " 'לשאלה': 1597,\n",
       " 'תחת': 1594,\n",
       " 'יחד': 1593,\n",
       " 'הכספים': 1591,\n",
       " 'נזק': 1589,\n",
       " 'בערעור': 1588,\n",
       " 'משרד': 1578,\n",
       " 'וראו': 1576,\n",
       " 'קיימת': 1574,\n",
       " 'החקירה': 1564,\n",
       " 'בפסיקה': 1562,\n",
       " 'שהיה': 1560,\n",
       " 'לפקודה': 1560,\n",
       " 'בעוד': 1559,\n",
       " 'השנים': 1557,\n",
       " 'יתר': 1550,\n",
       " 'העבירות': 1547,\n",
       " 'הצורך': 1545,\n",
       " 'טרם': 1541,\n",
       " 'עומדת': 1541,\n",
       " 'יכולה': 1540,\n",
       " 'אז': 1538,\n",
       " 'רבים': 1531,\n",
       " 'צורך': 1523,\n",
       " 'מצד': 1522,\n",
       " 'משפחה': 1519,\n",
       " 'בעבירת': 1518,\n",
       " 'ככלל': 1517,\n",
       " 'הקשר': 1515,\n",
       " 'הכנסת': 1509,\n",
       " 'האישור': 1503,\n",
       " 'מקרים': 1496,\n",
       " 'במועד': 1495,\n",
       " 'אמנם': 1494,\n",
       " 'שתי': 1493,\n",
       " 'לפקודת': 1492,\n",
       " 'להליך': 1490,\n",
       " 'ביטוח': 1489,\n",
       " 'טוען': 1487,\n",
       " 'לעומת': 1486,\n",
       " 'איסור': 1484,\n",
       " 'קובע': 1477,\n",
       " 'חובת': 1476,\n",
       " 'הוגשה': 1467,\n",
       " 'לישראל': 1467,\n",
       " 'הנכס': 1464,\n",
       " 'צד': 1463,\n",
       " 'א\"רע': 1463,\n",
       " 'עליה': 1460,\n",
       " 'שונה': 1455,\n",
       " 'קיומו': 1453,\n",
       " 'יצוין': 1452,\n",
       " 'המקומית': 1450,\n",
       " 'לעמוד': 1446,\n",
       " 'כללי': 1439,\n",
       " 'טען': 1438,\n",
       " 'הדירה': 1438,\n",
       " 'מסוג': 1437,\n",
       " 'אחרות': 1434,\n",
       " 'מכן': 1434,\n",
       " 'בדין': 1433,\n",
       " 'המשפטית': 1433,\n",
       " 'מסוים': 1430,\n",
       " 'מצב': 1428,\n",
       " 'להתערב': 1427,\n",
       " 'איני': 1427,\n",
       " 'סעיפים': 1425,\n",
       " 'הזכות': 1425,\n",
       " 'רב': 1419,\n",
       " 'הלכה': 1419,\n",
       " 'גזר': 1419,\n",
       " 'הענישה': 1417,\n",
       " 'בעבירה': 1416,\n",
       " 'קביעת': 1415,\n",
       " 'והוא': 1413,\n",
       " 'דעתי': 1413,\n",
       " 'קשה': 1413,\n",
       " 'בסך': 1407,\n",
       " 'עמדת': 1407,\n",
       " 'סבורה': 1403,\n",
       " 'מכאן': 1402,\n",
       " 'כגון': 1396,\n",
       " 'כבית': 1395,\n",
       " 'המעשה': 1393,\n",
       " 'הנאמנות': 1392,\n",
       " 'חודשים': 1387,\n",
       " 'מושא': 1383,\n",
       " 'לידי': 1382,\n",
       " 'שנות': 1381,\n",
       " 'הכלל': 1380,\n",
       " 'כתוצאה': 1379,\n",
       " 'נושא': 1373,\n",
       " 'ליתן': 1373,\n",
       " 'שומה': 1373,\n",
       " 'שאלה': 1370,\n",
       " 'דינה': 1369,\n",
       " 'בחוות': 1365,\n",
       " 'בפרט': 1364,\n",
       " \"'מס\": 1362,\n",
       " 'לכלל': 1361,\n",
       " 'ש\"היועמ': 1359,\n",
       " 'הזמן': 1358,\n",
       " 'פעולה': 1358,\n",
       " 'משכך': 1358,\n",
       " 'תשלום': 1357,\n",
       " 'בשים': 1356,\n",
       " 'הורשע': 1348,\n",
       " 'הדיונית': 1344,\n",
       " 'דחה': 1343,\n",
       " 'הציבורי': 1343,\n",
       " 'לכאורה': 1342,\n",
       " 'לבסוף': 1340,\n",
       " 'ראיה': 1336,\n",
       " 'עובר': 1333,\n",
       " 'הקבוע': 1332,\n",
       " 'החל': 1329,\n",
       " 'המאסר': 1328,\n",
       " 'מיסוי': 1328,\n",
       " 'והשוו': 1326,\n",
       " 'הפרשנות': 1325,\n",
       " 'עילה': 1323,\n",
       " 'דברי': 1317,\n",
       " 'חל': 1316,\n",
       " 'סכום': 1314,\n",
       " 'הצו': 1310,\n",
       " 'האמורה': 1308,\n",
       " 'מהם': 1306,\n",
       " 'הוסיף': 1305,\n",
       " 'הסעיף': 1304,\n",
       " 'פרסום': 1304,\n",
       " 'כספי': 1296,\n",
       " 'במקום': 1293,\n",
       " 'נוספות': 1291,\n",
       " 'בשבתו': 1291,\n",
       " 'הסמכות': 1291,\n",
       " 'ניתנה': 1290,\n",
       " 'היועץ': 1290,\n",
       " 'עילת': 1285,\n",
       " 'אתר': 1285,\n",
       " 'ההמתה': 1285,\n",
       " 'תכלית': 1284,\n",
       " 'לדון': 1281,\n",
       " 'מפני': 1279,\n",
       " 'מעשיו': 1276,\n",
       " 'חרף': 1276,\n",
       " 'כנגד': 1275,\n",
       " 'ר\"ד': 1275,\n",
       " 'מול': 1272,\n",
       " 'קושי': 1269,\n",
       " 'חברתי': 1267,\n",
       " 'ביטוי': 1267,\n",
       " 'השומה': 1266,\n",
       " 'דין-פסק': 1264,\n",
       " 'פעולות': 1264,\n",
       " 'הפרסום': 1260,\n",
       " 'בקשר': 1258,\n",
       " 'לתת': 1255,\n",
       " 'בעבר': 1254,\n",
       " 'ברור': 1254,\n",
       " 'חוזה': 1254,\n",
       " 'ג\"התשפ': 1252,\n",
       " 'לבצע': 1249,\n",
       " 'בתחום': 1246,\n",
       " 'החוסכים': 1245,\n",
       " 'בשאלה': 1242,\n",
       " 'כזה': 1241,\n",
       " 'לומר': 1240,\n",
       " 'משפטית': 1240,\n",
       " 'מדיניות': 1237,\n",
       " 'הישראלי': 1236,\n",
       " 'עומד': 1235,\n",
       " 'וכך': 1235,\n",
       " 'המקרקעין': 1233,\n",
       " 'הגנה': 1231,\n",
       " 'האזרחי': 1231,\n",
       " 'לביצוע': 1227,\n",
       " 'לנשיאה': 1227,\n",
       " 'לקבלת': 1223,\n",
       " 'חשש': 1223,\n",
       " 'לסעיף': 1223,\n",
       " 'בעתירה': 1222,\n",
       " 'החוב': 1220,\n",
       " 'לענייננו': 1220,\n",
       " 'הצבאי': 1219,\n",
       " 'הון': 1217,\n",
       " 'בא': 1216,\n",
       " 'סעד': 1215,\n",
       " 'בבקשה': 1212,\n",
       " 'פקיד': 1212,\n",
       " 'הקצה': 1212,\n",
       " 'בטרם': 1209,\n",
       " 'הנסיבות': 1204,\n",
       " 'המבחן': 1203,\n",
       " 'תקנות': 1203,\n",
       " 'כולל': 1200,\n",
       " 'חדלות': 1198,\n",
       " 'המשיבות': 1198,\n",
       " 'הגיע': 1193,\n",
       " 'ההסכם': 1193,\n",
       " 'אי': 1191,\n",
       " 'ממילא': 1187,\n",
       " 'טיפול': 1185,\n",
       " 'להוכיח': 1182,\n",
       " 'הפנים': 1182,\n",
       " 'שט': 1182,\n",
       " 'ממשי': 1178,\n",
       " 'לפעול': 1177,\n",
       " 'אינטרנט': 1176,\n",
       " 'בכלל': 1176,\n",
       " 'שלוש': 1175,\n",
       " 'הגישה': 1172,\n",
       " 'עצמה': 1170,\n",
       " 'רבות': 1169,\n",
       " 'באותה': 1165,\n",
       " 'זיקה': 1165,\n",
       " 'למנוע': 1160,\n",
       " 'הערר': 1159,\n",
       " \"'טל\": 1156,\n",
       " 'המבקשת': 1156,\n",
       " 'תובענה': 1156,\n",
       " 'במידה': 1154,\n",
       " 'קודם': 1147,\n",
       " 'עתירה': 1145,\n",
       " 'נדחתה': 1144,\n",
       " 'עולם': 1143,\n",
       " 'אישור': 1142,\n",
       " 'במשטרה': 1140,\n",
       " 'דווקא': 1138,\n",
       " 'הקופה': 1137,\n",
       " 'הבדיקה': 1136,\n",
       " 'חריגים': 1135,\n",
       " 'אנו': 1135,\n",
       " 'העסקה': 1130,\n",
       " 'העבודה': 1128,\n",
       " 'ייצוגיות': 1125,\n",
       " 'חקירה': 1123,\n",
       " 'ובכלל': 1122,\n",
       " 'חולק': 1121,\n",
       " 'הליכים': 1118,\n",
       " 'נראה': 1117,\n",
       " 'השנייה': 1117,\n",
       " 'מסקנה': 1116,\n",
       " 'הילד': 1116,\n",
       " 'דעתה': 1116,\n",
       " 'ביקש': 1115,\n",
       " 'נכון': 1114,\n",
       " 'נמצא': 1114,\n",
       " 'מנגד': 1112,\n",
       " 'שנקבעו': 1111,\n",
       " 'היסוד': 1108,\n",
       " 'לדיון': 1108,\n",
       " 'מגורים': 1108,\n",
       " 'שר': 1105,\n",
       " 'קרי': 1104,\n",
       " 'משנה': 1103,\n",
       " 'שהם': 1103,\n",
       " 'עת': 1103,\n",
       " 'א\"בע': 1102,\n",
       " 'החולים': 1101,\n",
       " 'הגיש': 1100,\n",
       " 'שניתנה': 1097,\n",
       " 'מתחם': 1097,\n",
       " 'המצב': 1096,\n",
       " 'הללו': 1095,\n",
       " 'בביצוע': 1095,\n",
       " 'לתיקון': 1094,\n",
       " 'שעל': 1093,\n",
       " 'הנושים': 1093,\n",
       " 'קביעה': 1092,\n",
       " 'עליהם': 1092,\n",
       " 'בדיון': 1091,\n",
       " 'טוענת': 1089,\n",
       " 'כוח': 1089,\n",
       " 'שאינה': 1089,\n",
       " 'הגם': 1086,\n",
       " 'זכאי': 1085,\n",
       " 'זוג': 1084,\n",
       " 'ממנה': 1081,\n",
       " 'משפחתו': 1078,\n",
       " 'במיוחד': 1077,\n",
       " 'רוכשי': 1076,\n",
       " 'הורה': 1075,\n",
       " 'נאשם': 1073,\n",
       " 'התנאים': 1073,\n",
       " 'שלה': 1072,\n",
       " 'תובענות': 1072,\n",
       " 'איננה': 1072,\n",
       " 'היקף': 1071,\n",
       " 'בחינת': 1071,\n",
       " 'פירעון': 1069,\n",
       " 'רואה': 1068,\n",
       " 'הליכי': 1068,\n",
       " 'ממועד': 1068,\n",
       " 'הסיכון': 1065,\n",
       " 'ולאחר': 1064,\n",
       " 'משמעותי': 1062,\n",
       " 'חוסר': 1060,\n",
       " 'פלילי': 1059,\n",
       " 'ואשר': 1058,\n",
       " 'שנית': 1058,\n",
       " 'בחשבון': 1058,\n",
       " 'להכריע': 1057,\n",
       " 'בניכוי': 1057,\n",
       " 'גביית': 1056,\n",
       " 'פלונית': 1055,\n",
       " 'מצאתי': 1054,\n",
       " 'אליה': 1054,\n",
       " 'בתיק': 1052,\n",
       " 'חדש': 1051,\n",
       " 'לפיו': 1050,\n",
       " 'ביטול': 1050,\n",
       " 'בהתחשב': 1049,\n",
       " 'המסקנה': 1049,\n",
       " 'האדם': 1049,\n",
       " 'התוצאה': 1048,\n",
       " 'חוב': 1048,\n",
       " 'אליו': 1047,\n",
       " 'נוסח': 1047,\n",
       " 'שינוי': 1047,\n",
       " 'והיא': 1045,\n",
       " 'משפטי': 1045,\n",
       " 'הלידה': 1042,\n",
       " 'לפגוע': 1041,\n",
       " \"עמית'\": 1041,\n",
       " 'במקרקעין': 1041,\n",
       " 'הנשיאה': 1040,\n",
       " 'סבורני': 1038,\n",
       " 'מערכת': 1033,\n",
       " 'הנישום': 1033,\n",
       " 'חולים': 1033,\n",
       " 'להטיל': 1031,\n",
       " 'להניח': 1029,\n",
       " 'מסכים': 1026,\n",
       " 'להוסיף': 1026,\n",
       " 'ועד': 1026,\n",
       " 'פעילות': 1025,\n",
       " 'המכר': 1025,\n",
       " 'ההורים': 1024,\n",
       " 'עשוי': 1022,\n",
       " 'של\"': 1022,\n",
       " 'הקובע': 1022,\n",
       " 'העירייה': 1022,\n",
       " 'עשויה': 1017,\n",
       " 'בבחינת': 1016,\n",
       " 'ואילו': 1015,\n",
       " 'תביעת': 1014,\n",
       " 'ההליכים': 1012,\n",
       " 'הכרעת': 1011,\n",
       " 'אירוע': 1011,\n",
       " 'המקרים': 1009,\n",
       " 'לגבות': 1009,\n",
       " 'להגשת': 1008,\n",
       " 'המומחה': 1008,\n",
       " 'רבה': 1007,\n",
       " 'המועצה': 1002,\n",
       " 'מסוימת': 1000,\n",
       " 'להבדיל': 999,\n",
       " 'שבהן': 997,\n",
       " 'ניכוי': 996,\n",
       " 'עדות': 995,\n",
       " 'צריך': 995,\n",
       " 'שבית': 994,\n",
       " 'יחסי': 994,\n",
       " 'היחסים': 991,\n",
       " 'בנושא': 990,\n",
       " 'לציין': 990,\n",
       " 'מצא': 989,\n",
       " 'החוזה': 989,\n",
       " 'בהוצאות': 988,\n",
       " 'הסרט': 987,\n",
       " 'המשקיעים': 986,\n",
       " 'לשלם': 986,\n",
       " 'מדוע': 985,\n",
       " 'ידו': 985,\n",
       " 'מחוץ': 984,\n",
       " 'עיון': 983,\n",
       " 'משמעות': 983,\n",
       " 'איננו': 982,\n",
       " 'טעם': 981,\n",
       " 'סדר': 979,\n",
       " 'מראש': 975,\n",
       " 'שיקולים': 975,\n",
       " 'הרפורמה': 975,\n",
       " 'במס': 974,\n",
       " 'יוצא': 973,\n",
       " 'ציבור': 973,\n",
       " 'המשפחה': 973,\n",
       " 'חלה': 973,\n",
       " 'הממשלה': 973,\n",
       " 'עקב': 970,\n",
       " 'הנוסף': 970,\n",
       " 'ראו\"': 967,\n",
       " 'ייצוגית': 967,\n",
       " 'בעמוד': 966,\n",
       " 'דמארי': 966,\n",
       " 'לאורך': 965,\n",
       " 'ביצע': 965,\n",
       " 'הוחלט': 965,\n",
       " 'קבוצת': 965,\n",
       " 'כזו': 964,\n",
       " 'למועד': 962,\n",
       " 'המעשים': 962,\n",
       " 'הוספה': 961,\n",
       " 'ניסיון': 961,\n",
       " 'בקנה': 961,\n",
       " 'התובענה': 959,\n",
       " 'הצדקה': 958,\n",
       " 'ללמוד': 958,\n",
       " 'מעשי': 958,\n",
       " 'הטענות': 958,\n",
       " 'העובדות': 957,\n",
       " 'אילו': 954,\n",
       " 'המפרקים': 954,\n",
       " 'רישום': 953,\n",
       " 'טוענים': 953,\n",
       " 'לביטוח': 953,\n",
       " 'אצל': 952,\n",
       " 'ברם': 952,\n",
       " 'בטענה': 950,\n",
       " 'במפורש': 950,\n",
       " 'בנכס': 949,\n",
       " 'השונים': 949,\n",
       " 'החלטות': 948,\n",
       " 'פ\"בע': 948,\n",
       " 'ורק': 948,\n",
       " 'לפגיעה': 948,\n",
       " 'מענה': 948,\n",
       " 'בסמוך': 948,\n",
       " 'גבוה': 948,\n",
       " 'הנפשי': 948,\n",
       " 'כהוצאה': 948,\n",
       " 'תום': 947,\n",
       " 'הביטחון': 947,\n",
       " 'פעם': 946,\n",
       " 'במשך': 945,\n",
       " 'פי-על': 944,\n",
       " 'מחמירות': 943,\n",
       " 'ובפרט': 943,\n",
       " 'בת': 943,\n",
       " 'במרמה': 942,\n",
       " 'אבו': 941,\n",
       " 'רשויות': 941,\n",
       " 'מסר': 938,\n",
       " 'קיומה': 938,\n",
       " \"שטיין'\": 937,\n",
       " 'נתן': 937,\n",
       " 'תוקף': 936,\n",
       " 'ערכאת': 935,\n",
       " 'בקשות': 934,\n",
       " 'ולכן': 934,\n",
       " 'לכן': 934,\n",
       " 'למעלה': 932,\n",
       " 'חשיבות': 932,\n",
       " 'סעיף\"': 932,\n",
       " 'שאינם': 932,\n",
       " 'נקודת': 931,\n",
       " 'הערכאה': 929,\n",
       " 'אותן': 928,\n",
       " 'מבחינה': 928,\n",
       " 'הקטינה': 928,\n",
       " 'ל\"הנ': 927,\n",
       " 'לביטול': 927,\n",
       " 'להוראות': 927,\n",
       " 'המוצא': 927,\n",
       " 'טעות': 926,\n",
       " 'לאמור': 925,\n",
       " 'לי': 925,\n",
       " 'כעת': 925,\n",
       " 'בזכות': 925,\n",
       " 'שביצע': 924,\n",
       " 'מחמת': 924,\n",
       " 'דהיינו': 924,\n",
       " 'סבירה': 923,\n",
       " 'למשיב': 923,\n",
       " 'הנאמן': 923,\n",
       " 'חודשי': 920,\n",
       " 'הישיבה': 918,\n",
       " 'קיבל': 916,\n",
       " 'אציע': 916,\n",
       " 'פגם': 916,\n",
       " 'השירות': 915,\n",
       " 'כיצד': 914,\n",
       " 'שנה': 914,\n",
       " 'הוראה': 911,\n",
       " 'וגם': 910,\n",
       " 'הזכויות': 910,\n",
       " 'בעלת': 909,\n",
       " 'טענו': 908,\n",
       " 'עורך': 907,\n",
       " 'הקבוצה': 903,\n",
       " 'עונשו': 902,\n",
       " 'לשיטת': 901,\n",
       " 'במובן': 901,\n",
       " 'מנהל': 900,\n",
       " 'עוסק': 898,\n",
       " 'התכנון': 898,\n",
       " 'שנגרם': 897,\n",
       " 'מעין': 894,\n",
       " 'ברי': 890,\n",
       " 'חיים': 890,\n",
       " 'עצם': 889,\n",
       " 'מצג': 888,\n",
       " 'בזמן': 887,\n",
       " 'הגישו': 887,\n",
       " 'לשלול': 885,\n",
       " 'ביניהם': 884,\n",
       " 'מבקש': 882,\n",
       " 'עבר': 881,\n",
       " 'למרות': 880,\n",
       " 'וממילא': 879,\n",
       " 'בתום': 878,\n",
       " 'שכר': 878,\n",
       " 'ראש': 877,\n",
       " 'ההדגשה': 876,\n",
       " 'כספים': 875,\n",
       " 'כה': 875,\n",
       " 'לערעורים': 872,\n",
       " 'העותרות': 872,\n",
       " 'שנת': 871,\n",
       " 'ידע': 871,\n",
       " 'בהחלטה': 870,\n",
       " 'הודעה': 869,\n",
       " 'עבודה': 869,\n",
       " 'נועד': 868,\n",
       " 'חוזר': 866,\n",
       " 'מכלול': 865,\n",
       " 'המחלוקת': 863,\n",
       " 'הוצאה': 863,\n",
       " 'המנהל': 862,\n",
       " 'מ\"מע': 862,\n",
       " 'הקטין': 862,\n",
       " 'לספק': 861,\n",
       " 'אודות': 861,\n",
       " 'הוכח': 859,\n",
       " 'לעמדת': 859,\n",
       " 'ברורה': 857,\n",
       " 'חילוט': 856,\n",
       " 'הבריאות': 855,\n",
       " 'מבחינת': 854,\n",
       " 'לטעמי': 853,\n",
       " 'הצרכן': 853,\n",
       " 'להכיר': 852,\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_idf_dict = get_idf_dict(data_gen,queries)\n",
    "my_idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tag_lookup_table(ner_labels):\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = [\"<PAD>\", \"O\",\"<NEXT>\"] + all_labels\n",
    "    return dict(zip(range(0, len(all_labels) + 1), all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: 'O', 2: '<NEXT>', 3: 'B-LAWYER', 4: 'I-LAWYER', 5: 'B-JUDGE', 6: 'I-JUDGE', 7: 'B-CASE', 8: 'I-CASE', 9: 'B-PROCEDURE', 10: 'I-PROCEDURE', 11: 'B-DEFENSE_TITLE', 12: 'I-DEFENSE_TITLE', 13: 'B-DEFENSE_PARTY', 14: 'I-DEFENSE_PARTY', 15: 'B-DEFENSE_LAWYER', 16: 'I-DEFENSE_LAWYER', 17: 'B-PROSECUTION_TITLE', 18: 'I-PROSECUTION_TITLE', 19: 'B-PROSECUTION_PARTY', 20: 'I-PROSECUTION_PARTY', 21: 'B-PROSECUTION_LAWYER', 22: 'I-PROSECUTION_LAWYER', 23: 'B-DATE', 24: 'I-DATE'}\n",
      "{'<PAD>': 0, 'O': 1, '<NEXT>': 2, 'B-LAWYER': 3, 'I-LAWYER': 4, 'B-JUDGE': 5, 'I-JUDGE': 6, 'B-CASE': 7, 'I-CASE': 8, 'B-PROCEDURE': 9, 'I-PROCEDURE': 10, 'B-DEFENSE_TITLE': 11, 'I-DEFENSE_TITLE': 12, 'B-DEFENSE_PARTY': 13, 'I-DEFENSE_PARTY': 14, 'B-DEFENSE_LAWYER': 15, 'I-DEFENSE_LAWYER': 16, 'B-PROSECUTION_TITLE': 17, 'I-PROSECUTION_TITLE': 18, 'B-PROSECUTION_PARTY': 19, 'I-PROSECUTION_PARTY': 20, 'B-PROSECUTION_LAWYER': 21, 'I-PROSECUTION_LAWYER': 22, 'B-DATE': 23, 'I-DATE': 24}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19999"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "NER_LAWYER = \"LAWYER\"\n",
    "NER_JUDGE = \"JUDGE\"\n",
    "NER_CASE = \"CASE\" \n",
    "NER_PROCEDURE = \"PROCEDURE\"\n",
    "NER_DEFENSE_TITLE = \"DEFENSE_TITLE\"\n",
    "NER_DEFENSE_PARTY = \"DEFENSE_PARTY\"\n",
    "NER_DEFENSE_LAWYER = \"DEFENSE_LAWYER\"\n",
    "NER_PROSECUTION_TITLE = \"PROSECUTION_TITLE\"\n",
    "NER_PROSECUTION_PARTY = \"PROSECUTION_PARTY\"\n",
    "NER_PROSECUTION_LAWYER = \"PROSECUTION_LAWYER\"\n",
    "NER_VERDICT_DATE = \"DATE\"\n",
    "NER_OTHER = \"O\"\n",
    "\n",
    "my_ner_labels = [\n",
    "        NER_LAWYER,\n",
    "        NER_JUDGE,\n",
    "        NER_CASE,\n",
    "        NER_PROCEDURE,\n",
    "        NER_DEFENSE_TITLE,\n",
    "        NER_DEFENSE_PARTY,\n",
    "        NER_DEFENSE_LAWYER,\n",
    "        NER_PROSECUTION_TITLE,\n",
    "        NER_PROSECUTION_PARTY,\n",
    "        NER_PROSECUTION_LAWYER,\n",
    "        NER_VERDICT_DATE,\n",
    "    ]\n",
    "\n",
    "ner_mapping = make_tag_lookup_table(my_ner_labels)\n",
    "keys = list(ner_mapping.keys())\n",
    "values = list(ner_mapping.values())\n",
    "reverse_ner_mapping = {values[i]:keys[i] for i in range(len(keys))}\n",
    "print(ner_mapping)\n",
    "print(reverse_ner_mapping)\n",
    "\n",
    "\n",
    "MAX_WORDS = 20000\n",
    "vocab = list(set(list(my_idf_dict.keys())))[:MAX_WORDS - 2]\n",
    "vocab.append(\"unknown\") \n",
    "vocab_idx2token = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "vocab_token2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "\n",
    "len(vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(lst_str:str):\n",
    "    return lst_str.split(\"|\")\n",
    "\n",
    "def string_to_nested_list(lst_str:str):\n",
    "    lst = []\n",
    "    for item in lst_str.split(\"@\"):\n",
    "        lst.append(string_to_list(item))\n",
    "    return lst\n",
    "\n",
    "\n",
    "def get_raw_bag_of_words(text:str):\n",
    "    t = remove_extra_spaces(text).strip()\n",
    "    words = t.split(\" \")\n",
    "    words = [word.strip() for word in words if len(word.strip()) > 0]\n",
    "    return words\n",
    "\n",
    "\n",
    "def split_and_pad_string(input_string, chunk_size, padding_character):\n",
    "    chunks = [input_string[i:i+chunk_size] for i in range(0, len(input_string), chunk_size)]\n",
    "    last_chunk_length = len(chunks[-1])\n",
    "    if padding_character:\n",
    "        if last_chunk_length < chunk_size:\n",
    "            chunks[-1] = chunks[-1].ljust(chunk_size, padding_character)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_sentences(text,size,split_string=\".\", padding_character=\" \"):\n",
    "    bag = get_raw_bag_of_words(text)\n",
    "    text = ' '.join(bag)\n",
    "    sentences = text.split(split_string)\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 1]\n",
    "    sentences = [s.strip()[:len(s)- 1].strip() if s[len(s) - 1].isdigit() else s for s in sentences]\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > size]\n",
    "    sentences = [split_and_pad_string(s,size,padding_character) for s in sentences]\n",
    "\n",
    "    combined_list_sentences = []\n",
    "    for sen in sentences:\n",
    "        combined_list_sentences.extend(sen)\n",
    "    return combined_list_sentences\n",
    "    \n",
    "\n",
    "def replace_pattern_to_ner_label(text,ner_label,multi_word_expression:str):\n",
    "    expression = multi_word_expression\n",
    "    new_text = text\n",
    "    label_text = \" \"\n",
    "    num_words_in_expression = len(expression.split(\" \"))\n",
    "    for word in range(num_words_in_expression):\n",
    "        label_text += ner_label + \" \"\n",
    "    label_text = label_text.strip()\n",
    "    pattern = re.compile(r\"\\b\" + re.escape(expression) + r\"\\b\")\n",
    "    modified_text = re.sub(pattern,label_text,new_text)\n",
    "    if modified_text != text:\n",
    "        return modified_text  \n",
    "    print(\"not changed\")      \n",
    "    return new_text\n",
    "\n",
    "\n",
    "def replace_patterns_to_ner_label(text,ner_label,expressions):\n",
    "    new_text = text\n",
    "    if isinstance(expressions,list):\n",
    "        for expressions in expressions:\n",
    "            new_text = replace_pattern_to_ner_label(new_text,ner_label,expressions)\n",
    "        return new_text\n",
    "    if isinstance(expressions,str):\n",
    "        new_text =  replace_pattern_to_ner_label(new_text,ner_label,expressions)\n",
    "        return new_text\n",
    "    return new_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_judges_names(row): # done \n",
    "    honor_prefix = \"כבוד\"\n",
    "    female_judge_prefix =  \" \" +  \"השופטת\"\n",
    "    male_judge_prefix = \" \" +  \"השופט\"\n",
    "    first_names = row['judge_first_names']\n",
    "    last_names = row['judge_last_names']\n",
    "    judge_genders = row['judges_genders']\n",
    "    last = string_to_list(last_names)\n",
    "    first = string_to_list(first_names)\n",
    "    genders = string_to_list(judge_genders)\n",
    "    judges = [last[i] +\"' \"+ first[i] for i in range(len(first))]    \n",
    "    judges_with_genders_prefixes = None\n",
    "    judges_with_genders_prefixes_and_honor_prefixes = None\n",
    "    for i in range(len(first)):\n",
    "        if genders[i] == \"male\":\n",
    "            judges_with_genders_prefixes = [last[i] +\"' \"+ first[i] + male_judge_prefix for i in range(len(first))]    \n",
    "            judges_with_genders_prefixes_and_honor_prefixes = [j  +  \" \"  +  honor_prefix for j in judges_with_genders_prefixes] \n",
    "        if genders[i] == \"female\":\n",
    "            judges_with_genders_prefixes = [last[i] +\"' \"+ first[i] + female_judge_prefix for i in range(len(first))]    \n",
    "            judges_with_genders_prefixes_and_honor_prefixes = [j  +  \" \"  +  honor_prefix for j in judges_with_genders_prefixes] \n",
    "    return judges,judges_with_genders_prefixes, judges_with_genders_prefixes_and_honor_prefixes\n",
    "\n",
    "    \n",
    "\n",
    "def get_team_one_names(row):\n",
    "    team_one_names = row['team_one_names']\n",
    "    team_one_names = string_to_list(team_one_names)\n",
    "    team_one_names = [remove_extra_spaces_without_lines(p) for p in team_one_names]\n",
    "    team_one_names = [t for t in team_one_names if len(t.strip()) > 0]\n",
    "    return team_one_names\n",
    "\n",
    "def get_team_two_names(row):\n",
    "    team_two_names = row['team_two_names']\n",
    "    team_two_names = string_to_list(team_two_names)\n",
    "    team_two_names = [remove_extra_spaces_without_lines(p) for p in team_two_names]\n",
    "    team_two_names = [t for t in team_two_names if len(t.strip()) > 0]\n",
    "    return team_two_names\n",
    "\n",
    "def get_team_one_parties(row):\n",
    "    parties = row['team_one']\n",
    "    parties = string_to_nested_list(parties)\n",
    "    team_one_parties = []\n",
    "    for party in parties:\n",
    "        team_one_parties = [remove_extra_spaces_without_lines(p) for p in party]\n",
    "    team_one_parties = [t for t in team_one_parties if len(t.strip()) > 0]\n",
    "    return team_one_parties\n",
    "\n",
    "def get_team_two_parties(row):\n",
    "    parties = row['team_two']\n",
    "    parties = string_to_nested_list(parties)\n",
    "    team_two_parties = []\n",
    "    for party in parties:\n",
    "        team_two_parties = [remove_extra_spaces_without_lines(p) for p in party]    \n",
    "    team_two_parties = [t for t in team_two_parties if len(t.strip()) > 0]\n",
    "    return team_two_parties\n",
    "\n",
    "\n",
    "def get_team_one_lawyers_teams_names(row):\n",
    "    teams = row['team_one_lawyers_teams_names']\n",
    "    teams = string_to_list(teams)\n",
    "    teams = [remove_extra_spaces_without_lines(p) for p in teams]\n",
    "    teams = [t for t in teams if len(t.strip()) > 0]\n",
    "    return teams\n",
    "\n",
    "\n",
    "def get_team_one_lawyers(row):\n",
    "    lawyers = row['team_one_lawyers']\n",
    "    lawyers = string_to_nested_list(lawyers)\n",
    "    lawyers_parties = []\n",
    "    for party in lawyers:\n",
    "        lawyers_parties = [remove_extra_spaces_without_lines(p) for p in party]    \n",
    "    lawyers_parties = [t for t in lawyers_parties if len(t.strip()) > 0]\n",
    "    return lawyers_parties\n",
    "\n",
    "def get_team_two_lawyers_teams_names(row):\n",
    "    teams = row['team_two_lawyers_teams_names']\n",
    "    teams = string_to_list(teams)\n",
    "    teams = [remove_extra_spaces_without_lines(p) for p in teams]\n",
    "    teams = [t for t in teams if len(t.strip()) > 0]\n",
    "    return teams    \n",
    "\n",
    "def get_team_two_lawyers(row):\n",
    "    lawyers = row['team_two_lawyers']\n",
    "    lawyers = string_to_nested_list(lawyers)\n",
    "    lawyers_parties = []\n",
    "    for party in lawyers:\n",
    "        lawyers_parties = [remove_extra_spaces_without_lines(p) for p in party]    \n",
    "    lawyers_parties = [t for t in lawyers_parties if len(t.strip()) > 0]\n",
    "    return lawyers_parties   \n",
    "\n",
    "def get_all_lawyers_teams_names(row):\n",
    "    lawyers_teams = row['all_lawyers_teams_names']\n",
    "    lawyers_teams = string_to_list(lawyers_teams)\n",
    "    lawyers_teams = [remove_extra_spaces_without_lines(p) for p in lawyers_teams]\n",
    "    lawyers_teams = [t for t in lawyers_teams if len(t.strip()) > 0]\n",
    "    return lawyers_teams\n",
    "\n",
    "\n",
    "def get_all_lawyers(row):\n",
    "    lawyer_prefix ='ע\"וד'\n",
    "    all_lawyers = row['all_lawyers']\n",
    "    all_lawyers = string_to_nested_list(all_lawyers)\n",
    "    lawyers_parties = []\n",
    "    for party in all_lawyers:\n",
    "        lawyers_parties = [remove_extra_spaces_without_lines(p) for p in party]    \n",
    "    lawyers_parties = [t for t in lawyers_parties if len(t.strip()) > 0]\n",
    "    return lawyers_parties\n",
    "\n",
    "\n",
    "def get_procedures(row):\n",
    "    procedures = row[\"procedures\"]\n",
    "    procedures = string_to_list(procedures)\n",
    "    procedures = [remove_extra_spaces_without_lines(p) for p in procedures]\n",
    "    return procedures\n",
    "\n",
    "def get_date(row):\n",
    "    date = row['date']\n",
    "    date = date.strip()\n",
    "    return date\n",
    "\n",
    "\n",
    "def get_title(row):\n",
    "    title = row['title']\n",
    "    title = title.strip()\n",
    "    title = remove_extra_spaces(title)\n",
    "    return title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _words_to_ner_label(words,ner_label):\n",
    "    tokens = words.split(\" \")\n",
    "    ners = []\n",
    "    for i,token in enumerate(tokens):\n",
    "        if i == 0:\n",
    "            ners.append(\"B-\" + ner_label)\n",
    "        else:\n",
    "            ners.append(\"I-\" + ner_label)\n",
    "    return ners\n",
    "\n",
    "def words_to_ner_label(words,ner_label):\n",
    "    lst = []\n",
    "    if isinstance(words,list):\n",
    "        for l in lst:\n",
    "            ner_lst = _words_to_ner_label(l,ner_label)\n",
    "            ner_text = \" \".join(ner_lst)\n",
    "            lst.append(ner_text) \n",
    "            ner_text = \" \".join(lst)\n",
    "            return ner_text\n",
    "    if isinstance(words,str):\n",
    "        lst = _words_to_ner_label(words,ner_label)\n",
    "        ner_text = \" \".join(lst)\n",
    "        return ner_text\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_max_size(word_list, ner_labels,end_sentences=('.', '!', '?')):\n",
    "    sentence_tokens = []\n",
    "    label_tokens = []\n",
    "\n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "    best_sentences_size = -1\n",
    "\n",
    "    last = None\n",
    "    for word, label in zip(word_list, ner_labels):\n",
    "        if label.startswith(\"B-\"):\n",
    "            last = label[2:]\n",
    "        else:\n",
    "            if last and label == \"I-\" + last:\n",
    "                current_sentence.append(word)\n",
    "                current_labels.append(label)\n",
    "                continue\n",
    "            else:\n",
    "                last = None\n",
    "\n",
    "        current_sentence.append(word)\n",
    "        current_labels.append(label)\n",
    "\n",
    "        if best_sentences_size < len(current_sentence):\n",
    "            best_sentences_size = len(current_sentence)\n",
    "            \n",
    "        if word.endswith(end_sentences):\n",
    "            sentence_tokens.append(current_sentence)\n",
    "            label_tokens.append(current_labels)\n",
    "            current_sentence = []\n",
    "            current_labels = []          \n",
    "        \n",
    "\n",
    "\n",
    "    return best_sentences_size\n",
    "\n",
    "def create_sentences_with_padding(word_list, ner_labels, max_sentence_size,end_sentences=('.', '!', '?'),padding_value=['<PAD>'],next_label=\"<NEXT>\"):\n",
    "    sentence_tokens = []\n",
    "    label_tokens = []\n",
    "\n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "\n",
    "    last = None\n",
    "    for word, label in zip(word_list, ner_labels):\n",
    "        if label.startswith(\"B-\"):\n",
    "            last = label[2:]\n",
    "        else:\n",
    "            if last and label == \"I-\" + last:\n",
    "                current_sentence.append(word)\n",
    "                current_labels.append(label)\n",
    "                continue\n",
    "            else:\n",
    "                last = None\n",
    "\n",
    "        current_sentence.append(word)\n",
    "        current_labels.append(label)\n",
    "\n",
    "        if word.endswith(end_sentences):\n",
    "            if len(current_sentence) >= max_sentence_size:\n",
    "                sentence_tokens.append(current_sentence)\n",
    "                label_tokens.append(current_labels)\n",
    "                current_sentence = []\n",
    "                current_labels = []\n",
    "                continue\n",
    "\n",
    "        if len(current_sentence) >= max_sentence_size:\n",
    "            current_labels = current_labels[:len(current_labels) - 1]\n",
    "            current_sentence = current_sentence[:len(current_sentence) - 1]\n",
    "            current_sentence.append(next_label)\n",
    "            current_labels.append(next_label)\n",
    "            sentence_tokens.append(current_sentence)\n",
    "            label_tokens.append(current_labels)\n",
    "            current_sentence = [word]\n",
    "            current_labels = [label]\n",
    "            continue\n",
    "\n",
    "\n",
    "        \n",
    "    if current_sentence:\n",
    "        sentence_tokens.append(current_sentence)\n",
    "        label_tokens.append(current_labels)\n",
    "\n",
    "    padded_sentence_tokens = []\n",
    "    padded_label_tokens = []\n",
    "\n",
    "    for sentence, labels in zip(sentence_tokens, label_tokens):\n",
    "        padding_length = max_sentence_size - len(sentence)\n",
    "        padded_sentence = sentence + padding_value * padding_length\n",
    "        padded_labels = labels + padding_value * padding_length\n",
    "        padded_sentence_tokens.append(padded_sentence)\n",
    "        padded_label_tokens.append(padded_labels)\n",
    "\n",
    "    return padded_sentence_tokens, padded_label_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_text(items,text):\n",
    "    all_in_text = True\n",
    "    if isinstance(items,list):\n",
    "        for j in items:\n",
    "            jj = j\n",
    "            if isinstance(jj,list):\n",
    "                for item in jj:\n",
    "                    if item in text:\n",
    "                        print(\"in text:\",item)\n",
    "                    else:\n",
    "                        print(\"not in text:\",item)\n",
    "                        all_in_text = False\n",
    "            else:\n",
    "                if jj in text:\n",
    "                    print(\"in text:\",jj)\n",
    "                else:\n",
    "                    print(\"not in text:\",jj)\n",
    "                    all_in_text = False\n",
    "    else:\n",
    "        if items in text:\n",
    "            print(\"in text:\",items)\n",
    "        else:\n",
    "            print(\"not in text:\",items)\n",
    "            all_in_text = False\n",
    "    return all_in_text\n",
    "\n",
    "\n",
    "\n",
    "def _replace_expression_with_ner_label(text:str,expression:str,ner_label:str):\n",
    "    ners = words_to_ner_label(expression,ner_label)\n",
    "    if not ners:\n",
    "        return text\n",
    "    modified_text = text.replace(expression,ners)\n",
    "    return modified_text\n",
    "\n",
    "def replace_expression_with_ner_label(text:str,expressions:typing.Union[str,list,None],ner_label:str):\n",
    "    new_text = text\n",
    "    if expressions is None:\n",
    "        return new_text\n",
    "    if isinstance(expressions,list):\n",
    "        for expression in expressions:\n",
    "            new_text = _replace_expression_with_ner_label(new_text,expression,ner_label)\n",
    "\n",
    "    if isinstance(expressions,str):\n",
    "        new_text = _replace_expression_with_ner_label(new_text,expressions,ner_label)\n",
    "    return new_text\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def replace_ners(row,text):\n",
    "    date = get_date(row)\n",
    "    title = get_title(row)\n",
    "    proz = get_procedures(row)\n",
    "    judges , judges_with_gender_prefix , judges_with_gender_honor_prefix = get_judges_names(row)\n",
    "    lawyers = get_all_lawyers(row)\n",
    "\n",
    "    text_tokens = text.split(\" \")\n",
    "\n",
    "    ner_text = text\n",
    "    ner_text = replace_expression_with_ner_label(ner_text,date,NER_VERDICT_DATE)\n",
    "    ner_text = replace_expression_with_ner_label(ner_text,title,NER_CASE)\n",
    "    ner_text = replace_expression_with_ner_label(ner_text,proz,NER_PROCEDURE)\n",
    "    ner_text = replace_expression_with_ner_label(ner_text,judges_with_gender_honor_prefix,NER_JUDGE)\n",
    "    ner_text = replace_expression_with_ner_label(ner_text,judges_with_gender_prefix,NER_JUDGE)\n",
    "    ner_text = replace_expression_with_ner_label(ner_text,judges,NER_JUDGE)\n",
    "    ner_text = replace_expression_with_ner_label(ner_text,lawyers,NER_LAWYER)\n",
    "\n",
    "\n",
    "    tokens = ner_text.split(\" \")\n",
    "    ner_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in list(ner_mapping.values()):\n",
    "            ner_tokens.append(token)\n",
    "        else:\n",
    "            ner_tokens.append(NER_OTHER)\n",
    "    \n",
    "    if len(text_tokens) != len(ner_tokens):\n",
    "        raise Exception()\n",
    "\n",
    "    return text_tokens,ner_tokens\n",
    "\n",
    "\n",
    "def encode_ner_tags(batch_ner_tags):\n",
    "    new_tags = []\n",
    "    for ner_tags in batch_ner_tags:\n",
    "        new_tags.append([reverse_ner_mapping[tag] for tag in ner_tags])\n",
    "    return new_tags\n",
    "\n",
    "\n",
    "def encode_sentences(vocab, tokens):\n",
    "    keys = list(vocab.keys())\n",
    "    return [vocab[token] if token in keys else len(keys) - 1 for token in tokens]\n",
    "            \n",
    "\n",
    "\n",
    "def create_sentences_tokens_ners_labels_batch(row,max_len_sentences):\n",
    "    verdict_data_divider = \"דין-פסק\"\n",
    "    text = row['text']\n",
    "    \n",
    "    meta_text = text.split(verdict_data_divider)[0]\n",
    "    verdict_text = text.split(verdict_data_divider)[1]    \n",
    "\n",
    "    meta_text_tokens,meta_ner_tokens = replace_ners(row,meta_text) \n",
    "    data_text_tokens,data_ner_tokens = replace_ners(row,verdict_text) \n",
    "\n",
    "    m1 = get_sentences_max_size(meta_text_tokens,meta_ner_tokens,end_sentences=(\"\\n\",\" \"))\n",
    "    m2 = get_sentences_max_size(data_text_tokens,data_ner_tokens,end_sentences=('.','!','?'))\n",
    "\n",
    "\n",
    "    sen1 , lab1 = create_sentences_with_padding(meta_text_tokens,meta_ner_tokens,max_len_sentences,end_sentences=(\"\\n\",\" \"))\n",
    "    sen2 , lab2 = create_sentences_with_padding(data_text_tokens,data_ner_tokens,max_len_sentences,end_sentences=('.','!','?'))\n",
    "\n",
    "    sentences = sen1 + sen2\n",
    "    ners_tags = lab1 + lab2\n",
    "    \n",
    "    return sentences , ners_tags\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'תחייבנה': 0,\n",
       " 'עבר-חלף': 1,\n",
       " 'לבוחנן': 2,\n",
       " '300-ו': 3,\n",
       " 'יתלה': 4,\n",
       " 'לוינר': 5,\n",
       " 'אמצעי-בלתי': 6,\n",
       " 'על10': 7,\n",
       " 'ובסיוע': 8,\n",
       " 'יודגש\"': 9,\n",
       " 'מהאופן\"': 10,\n",
       " 'המורישים': 11,\n",
       " 'להעצמתם': 12,\n",
       " 'צדקה': 13,\n",
       " 'וכמי': 14,\n",
       " 'בהתנדבות': 15,\n",
       " 'נפגעת': 16,\n",
       " 'קפוטה': 17,\n",
       " 'יתרון92': 18,\n",
       " 'שהתיקונים': 19,\n",
       " 'מפוקחות': 20,\n",
       " 'שמופקדת': 21,\n",
       " 'עושרם': 22,\n",
       " 'שהסרטון': 23,\n",
       " \"פלנטה'ב\": 24,\n",
       " 'להשתתף–': 25,\n",
       " 'מבור': 26,\n",
       " 'פוסלת': 27,\n",
       " 'שנתייםוכ': 28,\n",
       " 'שהתמלאו': 29,\n",
       " 'בפסיקה6': 30,\n",
       " 'שהקשר': 31,\n",
       " 'פ\"בבש': 32,\n",
       " 'משמעשהו': 33,\n",
       " 'דקלים': 34,\n",
       " 'התשואות': 35,\n",
       " '1414-ל': 36,\n",
       " 'מותירות': 37,\n",
       " 'או9': 38,\n",
       " 'דיוקו': 39,\n",
       " 'מסקנותיו': 40,\n",
       " 'הפלורליסטיים': 41,\n",
       " 'להידבק': 42,\n",
       " 'שנעלם': 43,\n",
       " 'דירה-\"ל': 44,\n",
       " 'הדרכת': 45,\n",
       " 'חילוט-כברי': 46,\n",
       " 'ומופנה': 47,\n",
       " 'בכפוף18': 48,\n",
       " 'הנוסעים': 49,\n",
       " 'הביקורת\"': 50,\n",
       " 'יחמוק': 51,\n",
       " '\"הצבת': 52,\n",
       " 'והזוכה': 53,\n",
       " 'מישהו3': 54,\n",
       " 'לשפיטתו': 55,\n",
       " 'פני6': 56,\n",
       " 'משען': 57,\n",
       " 'להתקדמותו': 58,\n",
       " 'לצוואות': 59,\n",
       " 'וכחודש': 60,\n",
       " \"שציינה'\": 61,\n",
       " '\"הירושה': 62,\n",
       " 'יתנקם': 63,\n",
       " '\"שומעין': 64,\n",
       " 'קִצו': 65,\n",
       " 'המשקף\"': 66,\n",
       " 'ברוביקון': 67,\n",
       " 'ותוגש': 68,\n",
       " 'ומסומן': 69,\n",
       " 'ודיווחו': 70,\n",
       " 'תענית': 71,\n",
       " 'שרופאי': 72,\n",
       " 'שהזכויות': 73,\n",
       " 'זוליכאן': 74,\n",
       " '\"פינה': 75,\n",
       " 'עצמו-היזם': 76,\n",
       " 'גניבתו': 77,\n",
       " 'להכפיפם': 78,\n",
       " 'קיפוח': 79,\n",
       " 'שהקיזוז': 80,\n",
       " 'תוצדק': 81,\n",
       " 'כתיאורו': 82,\n",
       " \"'יִשום\": 83,\n",
       " 'רדעי': 84,\n",
       " 'בזכויותיהן': 85,\n",
       " 'אקצא-אל': 86,\n",
       " 'ממילות': 87,\n",
       " 'שנתגלו': 88,\n",
       " 'מרשיעים': 89,\n",
       " 'סבלנות': 90,\n",
       " 'אסדרתן': 91,\n",
       " 'ואיגוד': 92,\n",
       " 'פתורים': 93,\n",
       " 'במועדים': 94,\n",
       " 'המסתכם': 95,\n",
       " 'כצרכן': 96,\n",
       " 'כהגדרתן\"': 97,\n",
       " 'והגיונית': 98,\n",
       " 'משהמחוקק': 99,\n",
       " 'בובת\"ל': 100,\n",
       " 'אבנעל': 101,\n",
       " 'ומשאין': 102,\n",
       " 'עליונה7': 103,\n",
       " 'המצוי': 104,\n",
       " 'את12': 105,\n",
       " 'תוצהר': 106,\n",
       " 'המתואר\"': 107,\n",
       " 'התפשטו': 108,\n",
       " 'שההבדלים': 109,\n",
       " 'דרס': 110,\n",
       " 'משתיקתו': 111,\n",
       " 'ונחיתות': 112,\n",
       " 'אשרר': 113,\n",
       " 'ומתקיימת': 114,\n",
       " 'שמתייחסת': 115,\n",
       " 'כבישים14': 116,\n",
       " 'המופרד': 117,\n",
       " 'בתקופה5': 118,\n",
       " 'בהפקות': 119,\n",
       " 'וממורכבותה': 120,\n",
       " 'כשקרית': 121,\n",
       " 'מהקשרו': 122,\n",
       " '2010-ו': 123,\n",
       " 'היסוד11': 124,\n",
       " 'ותשתיתיים': 125,\n",
       " 'בהשגות': 126,\n",
       " 'שבצד': 127,\n",
       " 'מזכאי': 128,\n",
       " 'יקדיש': 129,\n",
       " 'בחודשיים': 130,\n",
       " 'והחסוי': 131,\n",
       " 'לגבותה': 132,\n",
       " 'ז\"תשנ': 133,\n",
       " \"'הגשת\": 134,\n",
       " 'הפירוט': 135,\n",
       " 'חברוני': 136,\n",
       " 'אבד': 137,\n",
       " 'המטרות': 138,\n",
       " '\"בפוזיציית': 139,\n",
       " 'לתושבים': 140,\n",
       " 'קרימינולוגיה': 141,\n",
       " 'ממאסדר': 142,\n",
       " 'האקטואר': 143,\n",
       " 'מיופי': 144,\n",
       " 'תרומתל\"': 145,\n",
       " 'עדותה10': 146,\n",
       " 'ווסתו': 147,\n",
       " 'המדינה3': 148,\n",
       " '\"במהלכה': 149,\n",
       " 'פיילוט': 150,\n",
       " 'דיני': 151,\n",
       " 'למפלגתו': 152,\n",
       " 'ר\"ד': 153,\n",
       " 'וההרתעה': 154,\n",
       " 'נתקן': 155,\n",
       " 'המשמעות': 156,\n",
       " 'מרשלנות': 157,\n",
       " 'לקודמו': 158,\n",
       " 'שפתית': 159,\n",
       " 'ההגבלות': 160,\n",
       " 'ובחג': 161,\n",
       " 'ידו-למשלח': 162,\n",
       " 'מנייה': 163,\n",
       " 'להתקרב': 164,\n",
       " 'ומצומצם': 165,\n",
       " '\"אלפריח': 166,\n",
       " 'בנעלי': 167,\n",
       " 'והגנים': 168,\n",
       " '\"ועמו': 169,\n",
       " '169ת': 170,\n",
       " 'ויודעת': 171,\n",
       " 'הארט': 172,\n",
       " 'הנפקתן': 173,\n",
       " 'סקוק': 174,\n",
       " 'תמריץ61': 175,\n",
       " 'בתחילתו': 176,\n",
       " 'שיסודו': 177,\n",
       " 'ובהרשאת': 178,\n",
       " 'נקיטת-ואי': 179,\n",
       " 'כשהעמידו': 180,\n",
       " 'אליו262': 181,\n",
       " 'מרכז32': 182,\n",
       " \"'וכו\": 183,\n",
       " 'נכון-הלא': 184,\n",
       " 'המנציחה': 185,\n",
       " '-מכ': 186,\n",
       " 'שיכל': 187,\n",
       " 'אישיותו': 188,\n",
       " '\"תיאורטיים': 189,\n",
       " 'לאומיים': 190,\n",
       " 'עסקה': 191,\n",
       " 'ומשפרת': 192,\n",
       " 'אחֵי': 193,\n",
       " 'במצבים': 194,\n",
       " \"מל'אלג\": 195,\n",
       " 'המוקנות': 196,\n",
       " 'לעסק': 197,\n",
       " 'להיחלשות': 198,\n",
       " 'מחבוּתה': 199,\n",
       " 'ג-ש': 200,\n",
       " \"טקסטיל'\": 201,\n",
       " 'ולהורגו': 202,\n",
       " 'לפרוטוקול3': 203,\n",
       " 'צמיגים': 204,\n",
       " 'נשואיו': 205,\n",
       " 'מבעיר': 206,\n",
       " 'יישומיות': 207,\n",
       " 'אאוּט-זוּם\"': 208,\n",
       " 'שצמוד': 209,\n",
       " 'בסכומי': 210,\n",
       " 'ששאלתי': 211,\n",
       " 'מאסדר': 212,\n",
       " 'סטאטוס': 213,\n",
       " 'אוכף': 214,\n",
       " 'סמונד': 215,\n",
       " '\"דבר-\"ה': 216,\n",
       " 'לבדיית': 217,\n",
       " 'ופרפרה': 218,\n",
       " 'נכסי\"מ': 219,\n",
       " 'תנא\"': 220,\n",
       " 'בהשערה': 221,\n",
       " 'שמתעדכנות': 222,\n",
       " 'המדממת': 223,\n",
       " 'גת-קריית': 224,\n",
       " 'החמורה': 225,\n",
       " '\"התיבות': 226,\n",
       " '\"בפרק': 227,\n",
       " 'ומההשוואה': 228,\n",
       " 'גיטל': 229,\n",
       " 'הראייתיים': 230,\n",
       " 'מהמטוש': 231,\n",
       " 'ותובני': 232,\n",
       " 'אוגדה': 233,\n",
       " 'קבוצת\"': 234,\n",
       " 'למשקיעים25': 235,\n",
       " 'מלהתלונן': 236,\n",
       " 'והורחב': 237,\n",
       " 'ומתי': 238,\n",
       " 'צנחה': 239,\n",
       " 'בהשתייכות': 240,\n",
       " 'מרכז95': 241,\n",
       " 'ותפקידיו': 242,\n",
       " 'פגיעתה': 243,\n",
       " 'סלוטייפ\"': 244,\n",
       " 'לאורו': 245,\n",
       " 'שמשתמש': 246,\n",
       " 'ה\"התשס': 247,\n",
       " 'כימיים': 248,\n",
       " 'להפחידם': 249,\n",
       " '\"יצריו': 250,\n",
       " 'התרמיל': 251,\n",
       " 'לזמינותו': 252,\n",
       " '\"המתחזים': 253,\n",
       " 'פגיעה35': 254,\n",
       " 'חילול': 255,\n",
       " 'ושתבצע': 256,\n",
       " 'ועיבוי': 257,\n",
       " '\"שהפרויקט': 258,\n",
       " 'ויגישו': 259,\n",
       " 'ובדם': 260,\n",
       " 'ממדובבים': 261,\n",
       " 'ושבבסיסם': 262,\n",
       " 'מכן': 263,\n",
       " 'כתחתית': 264,\n",
       " 'שאם–': 265,\n",
       " 'ובפעולה': 266,\n",
       " 'ובטיעוניו': 267,\n",
       " 'בדיונו': 268,\n",
       " 'הדחוף': 269,\n",
       " '6012-ל': 270,\n",
       " 'שנדונו': 271,\n",
       " 'עלילותיהם': 272,\n",
       " 'לאי': 273,\n",
       " 'שבפרק': 274,\n",
       " 'ויחסו': 275,\n",
       " '59ת': 276,\n",
       " 'באסלאם': 277,\n",
       " '60-מ': 278,\n",
       " '\"ואליום\"': 279,\n",
       " 'ק\"שבס': 280,\n",
       " 'ששורשיו': 281,\n",
       " '\"נדונה': 282,\n",
       " 'ומהמעיל': 283,\n",
       " 'המכתיבה': 284,\n",
       " 'ולהשבת': 285,\n",
       " 'לאישומים': 286,\n",
       " 'ישרת': 287,\n",
       " 'בהגבלתה': 288,\n",
       " 'וְגַם': 289,\n",
       " 'כדי16': 290,\n",
       " 'דחוף': 291,\n",
       " 'לבקשת9': 292,\n",
       " 'כשוליים': 293,\n",
       " 'בצמודי': 294,\n",
       " 'הגדה4': 295,\n",
       " 'נחוצה': 296,\n",
       " 'תכלכל': 297,\n",
       " 'חנוכה': 298,\n",
       " 'גוריון-בן': 299,\n",
       " 'עליאן': 300,\n",
       " 'לישיבות': 301,\n",
       " 'מקרה\"ב': 302,\n",
       " 'הירי': 303,\n",
       " \"ואינם'\": 304,\n",
       " 'ם\"רמב': 305,\n",
       " 'שהקו': 306,\n",
       " 'נגבית': 307,\n",
       " 'בסיומה': 308,\n",
       " 'בשמונה': 309,\n",
       " 'ולזמנים': 310,\n",
       " 'ואדהם': 311,\n",
       " '\"האמירה': 312,\n",
       " 'הסיבתי6': 313,\n",
       " 'תקיף': 314,\n",
       " 'תחלקנה': 315,\n",
       " 'ירחק': 316,\n",
       " 'גמול': 317,\n",
       " 'וריבי': 318,\n",
       " 'בני': 319,\n",
       " 'המגבילות24': 320,\n",
       " 'במסכות': 321,\n",
       " 'תמונות\"': 322,\n",
       " 'ולמצבה': 323,\n",
       " 'סגולותיו': 324,\n",
       " 'המעוותים': 325,\n",
       " 'המובעת': 326,\n",
       " 'השאלהקבע': 327,\n",
       " 'שנזקקו': 328,\n",
       " 'מצר': 329,\n",
       " 'משהנחנו': 330,\n",
       " 'תחזק': 331,\n",
       " 'הקולטים': 332,\n",
       " 'יפדה': 333,\n",
       " 'לחלופה': 334,\n",
       " 'מניסוחו': 335,\n",
       " 'בהגישם': 336,\n",
       " 'חגגו': 337,\n",
       " 'מחוייבים': 338,\n",
       " 'שקשה32': 339,\n",
       " 'במרוכז': 340,\n",
       " 'מניחה42': 341,\n",
       " 'שהשקעתם': 342,\n",
       " 'מבניה': 343,\n",
       " 'האוניברסלית': 344,\n",
       " '\"ההליך': 345,\n",
       " 'לשותפיו': 346,\n",
       " 'שסוג': 347,\n",
       " 'ותכליתו': 348,\n",
       " 'חזרתי': 349,\n",
       " 'ם\"יהל': 350,\n",
       " 'במשמעותו': 351,\n",
       " 'החזויות': 352,\n",
       " 'תוארך': 353,\n",
       " 'עשירים': 354,\n",
       " 'שבאותן': 355,\n",
       " 'יחסי15': 356,\n",
       " 'וייסלברג-צור': 357,\n",
       " 'הגנובות': 358,\n",
       " 'נוסד': 359,\n",
       " 'ומצדיק': 360,\n",
       " 'הפקדתה': 361,\n",
       " 'משילוט': 362,\n",
       " 'וובסטר': 363,\n",
       " 'במאפייניהן': 364,\n",
       " 'ככל34': 365,\n",
       " 'בקהילה': 366,\n",
       " 'בהיקפים': 367,\n",
       " 'התרשמותנו': 368,\n",
       " 'לשלום': 369,\n",
       " 'והעדרם': 370,\n",
       " 'המחוקקים': 371,\n",
       " 'והכספיים': 372,\n",
       " 'חרקים': 373,\n",
       " 'ולהתחייבויות': 374,\n",
       " 'מקורביו': 375,\n",
       " 'מצא\"': 376,\n",
       " 'מלכיאלי': 377,\n",
       " 'נידונים\"': 378,\n",
       " 'מישהו': 379,\n",
       " 'טיסות': 380,\n",
       " \"עסקה'\": 381,\n",
       " 'התכלית18': 382,\n",
       " 'אתמול': 383,\n",
       " 'מחדלי\"ל': 384,\n",
       " 'בחירת': 385,\n",
       " 'נפסלה': 386,\n",
       " 'שעליו': 387,\n",
       " '\"אליהם': 388,\n",
       " \"ויסמן'\": 389,\n",
       " 'ואמנת': 390,\n",
       " 'זוהיר': 391,\n",
       " 'מתקפל': 392,\n",
       " 'והפרטיים': 393,\n",
       " 'המובטחת': 394,\n",
       " 'ב\"תשכ': 395,\n",
       " '\"סתמי': 396,\n",
       " \"אינסל'\": 397,\n",
       " '\"למפלגת': 398,\n",
       " 'ט\"שכה': 399,\n",
       " '\"מתנות\"ב': 400,\n",
       " 'לפסוע': 401,\n",
       " 'החמורות': 402,\n",
       " 'דומני': 403,\n",
       " 'ב126ת': 404,\n",
       " 'לפדות\"': 405,\n",
       " 'שהולידה\"': 406,\n",
       " '\"כראי\"': 407,\n",
       " 'והשלמות': 408,\n",
       " 'שכנטען': 409,\n",
       " 'רשמנו': 410,\n",
       " 'לאימה': 411,\n",
       " 'פלסטינאים': 412,\n",
       " 'בסוריה': 413,\n",
       " 'להצדיקו': 414,\n",
       " 'לפגיעה\"': 415,\n",
       " 'התייצבותם': 416,\n",
       " 'ממוקדת': 417,\n",
       " 'לבישת': 418,\n",
       " '\"המונחים': 419,\n",
       " 'סלל': 420,\n",
       " 'לחגי': 421,\n",
       " 'ההחלטה23': 422,\n",
       " \"'תחרות-אי'ל\": 423,\n",
       " 'יוזמי': 424,\n",
       " 'ואישרר': 425,\n",
       " 'להתחלתו': 426,\n",
       " 'להשיא': 427,\n",
       " 'תחוס': 428,\n",
       " 'למתווכים': 429,\n",
       " '-תקבול': 430,\n",
       " 'ופסולה': 431,\n",
       " 'שמוענקות': 432,\n",
       " 'גובר': 433,\n",
       " 'המופקות': 434,\n",
       " 'לכריתת5': 435,\n",
       " '3נז': 436,\n",
       " 'ובהשלכותיה': 437,\n",
       " 'גאולת': 438,\n",
       " 'הסותרת9': 439,\n",
       " 'למטרה4': 440,\n",
       " 'להענישו': 441,\n",
       " '1042-ו': 442,\n",
       " 'ואנדריי': 443,\n",
       " 'והשיח': 444,\n",
       " 'פולגת': 445,\n",
       " 'ברינגר': 446,\n",
       " 'בתביעה': 447,\n",
       " '2023-ב': 448,\n",
       " 'השקר': 449,\n",
       " 'סוף-סוף': 450,\n",
       " 'חסין': 451,\n",
       " 'מהתנהגות': 452,\n",
       " 'כונו': 453,\n",
       " 'פילנטרופית': 454,\n",
       " 'תפעלנה': 455,\n",
       " 'סאטירה\"כ': 456,\n",
       " 'השמלה': 457,\n",
       " 'פנים-מול-פנים': 458,\n",
       " 'במעמדו': 459,\n",
       " 'חישובים': 460,\n",
       " 'דס': 461,\n",
       " 'במארזים': 462,\n",
       " 'וכפשוטו': 463,\n",
       " 'שאורכן': 464,\n",
       " 'שש3': 465,\n",
       " 'חורף': 466,\n",
       " 'שבליבת': 467,\n",
       " 'מנשיית': 468,\n",
       " 'ולהציגם': 469,\n",
       " 'פירט2': 470,\n",
       " '\\'\"המלך': 471,\n",
       " 'שבורות': 472,\n",
       " '\"הרווחה': 473,\n",
       " \"12קרא'\": 474,\n",
       " 'והדרגה': 475,\n",
       " 'משכנות': 476,\n",
       " 'פיראס': 477,\n",
       " 'תתלונן': 478,\n",
       " \"בינייש'\": 479,\n",
       " 'ולראשונה': 480,\n",
       " 'מניצולה': 481,\n",
       " 'יתן': 482,\n",
       " 'תשקף': 483,\n",
       " 'שצאצאיה': 484,\n",
       " 'עיקריה': 485,\n",
       " '\"האתיקה': 486,\n",
       " 'ולערכים': 487,\n",
       " 'ניתוק\"': 488,\n",
       " 'שההריסה': 489,\n",
       " 'ברינג': 490,\n",
       " 'נדרשות': 491,\n",
       " 'מתוכנם': 492,\n",
       " 'הדיה': 493,\n",
       " 'שתהפוך': 494,\n",
       " 'יקרה': 495,\n",
       " 'וממסמכים': 496,\n",
       " 'ערה\"': 497,\n",
       " 'מהפכנית': 498,\n",
       " 'שופכים': 499,\n",
       " 'לטויוטה': 500,\n",
       " 'מהפעלת\"': 501,\n",
       " 'שתניית': 502,\n",
       " 'השוויוני': 503,\n",
       " 'להצטרפותו': 504,\n",
       " 'ודומה25': 505,\n",
       " 'שלגישתי': 506,\n",
       " 'ירך': 507,\n",
       " 'מחוזי': 508,\n",
       " '185נ': 509,\n",
       " 'לגירושיהם': 510,\n",
       " 'קללה': 511,\n",
       " 'ר\"ומיו': 512,\n",
       " 'עורפיים': 513,\n",
       " 'ברכה': 514,\n",
       " 'הבדיונית': 515,\n",
       " 'שהאמור': 516,\n",
       " 'שמגישים': 517,\n",
       " 'כממנים': 518,\n",
       " 'ולהעמידו': 519,\n",
       " 'ריככה': 520,\n",
       " 'ולקצר': 521,\n",
       " '2020-א\"התשפ': 522,\n",
       " 'לנמקה': 523,\n",
       " 'קני': 524,\n",
       " 'רוחות': 525,\n",
       " '3ת': 526,\n",
       " 'והשליטה': 527,\n",
       " 'תהיו': 528,\n",
       " \"עברי'\": 529,\n",
       " 'אפתחא': 530,\n",
       " 'לחצרים': 531,\n",
       " 'ובאמון': 532,\n",
       " 'הכוחני': 533,\n",
       " 'וניצבים': 534,\n",
       " 'צרכנית-פרו': 535,\n",
       " 'ולמומחה': 536,\n",
       " 'וסייג': 537,\n",
       " 'ללכדו': 538,\n",
       " 'אז-או': 539,\n",
       " 'עבודתו': 540,\n",
       " 'והוכרזה': 541,\n",
       " '\"עסק': 542,\n",
       " 'ביומיומו': 543,\n",
       " 'עקב8': 544,\n",
       " 'המצריך': 545,\n",
       " 'עליה\"': 546,\n",
       " 'יוצאי': 547,\n",
       " 'מששה': 548,\n",
       " 'חברם': 549,\n",
       " 'שיכונן': 550,\n",
       " 'שאיני': 551,\n",
       " 'בסיכון': 552,\n",
       " 'טראומטי': 553,\n",
       " 'נבדלת': 554,\n",
       " 'וזכאות': 555,\n",
       " 'להֵצר': 556,\n",
       " 'המרינס': 557,\n",
       " 'בפני25': 558,\n",
       " 'הגיגים': 559,\n",
       " 'וכשנתיים': 560,\n",
       " 'לחשוב': 561,\n",
       " 'משאמרה': 562,\n",
       " 'המטאפורית': 563,\n",
       " 'להעריך': 564,\n",
       " 'מהותית9': 565,\n",
       " 'אירוע\"': 566,\n",
       " \"להכריע'\": 567,\n",
       " 'מסוממת': 568,\n",
       " 'והעיקרית': 569,\n",
       " '4-כ': 570,\n",
       " 'מאלצת': 571,\n",
       " 'ומלחץ': 572,\n",
       " 'להן': 573,\n",
       " 'באקספוז': 574,\n",
       " 'ממליצה': 575,\n",
       " 'בט40': 576,\n",
       " 'א95-ו': 577,\n",
       " 'לקצינת': 578,\n",
       " 'מתחילתו2': 579,\n",
       " '\"דין': 580,\n",
       " '15-מ': 581,\n",
       " '\"הקישור': 582,\n",
       " 'מלים': 583,\n",
       " 'המגיעה': 584,\n",
       " 'בראש': 585,\n",
       " 'שלעבירה': 586,\n",
       " 'נקטה': 587,\n",
       " 'למחות': 588,\n",
       " 'והמיובא': 589,\n",
       " 'שיפוטן': 590,\n",
       " 'ספירתה': 591,\n",
       " 'ובמועצת': 592,\n",
       " 'הם1': 593,\n",
       " 'החוקיות-אי': 594,\n",
       " 'שקט\"': 595,\n",
       " 'כשהתערב': 596,\n",
       " 'לתיקון': 597,\n",
       " 'והרגשות': 598,\n",
       " 'המחסן': 599,\n",
       " 'שהיחס': 600,\n",
       " 'שהקניטו': 601,\n",
       " 'דלומי': 602,\n",
       " 'ומקום': 603,\n",
       " 'במלאכותיות': 604,\n",
       " 'ע\"על': 605,\n",
       " 'שמציבים': 606,\n",
       " 'והוצאותיהם': 607,\n",
       " 'לגישת7': 608,\n",
       " 'ובניסיונו': 609,\n",
       " 'להמלצתה': 610,\n",
       " 'פורמים': 611,\n",
       " 'מוקנים': 612,\n",
       " 'להעתקת': 613,\n",
       " 'יליד': 614,\n",
       " 'אפסח': 615,\n",
       " 'במהותו\"': 616,\n",
       " \"רדאת'ג\": 617,\n",
       " 'הקצר': 618,\n",
       " 'שמפיקה': 619,\n",
       " 'ר\"לכנ': 620,\n",
       " 'מילת': 621,\n",
       " 'בהמצאת\"': 622,\n",
       " 'לכישורים': 623,\n",
       " 'השפיר': 624,\n",
       " 'עיוני\"': 625,\n",
       " 'גופו': 626,\n",
       " 'שבתשלום': 627,\n",
       " 'ההנחה\"': 628,\n",
       " 'ושפרטנר': 629,\n",
       " 'לכשתושלם': 630,\n",
       " 'הכרעותיו': 631,\n",
       " 'כמספרם': 632,\n",
       " 'המצריכה': 633,\n",
       " 'לנוהל14': 634,\n",
       " 'אגריפרם': 635,\n",
       " 'התשת': 636,\n",
       " 'המקומית20': 637,\n",
       " 'גמרה': 638,\n",
       " 'הפרסונאלי': 639,\n",
       " 'בהנהלתם': 640,\n",
       " 'בקירור': 641,\n",
       " 'שתנועת': 642,\n",
       " 'גישתי': 643,\n",
       " '\"אחריות': 644,\n",
       " 'שניהל': 645,\n",
       " 'לפעמיים': 646,\n",
       " 'אוראל': 647,\n",
       " 'ריכוך\"ל': 648,\n",
       " 'למילוט': 649,\n",
       " 'שאופיים': 650,\n",
       " 'לנוסעים': 651,\n",
       " 'מחקירותיו': 652,\n",
       " 'בתנועות': 653,\n",
       " 'כתכליתו': 654,\n",
       " 'המשיבה': 655,\n",
       " 'בסנוקר': 656,\n",
       " \"א'ואג'אלח\": 657,\n",
       " 'פרגמטיים': 658,\n",
       " 'הנאשמת-העותרת': 659,\n",
       " 'וליקוי': 660,\n",
       " 'שבמסמכי': 661,\n",
       " '2א47': 662,\n",
       " '\"\\'אלון\\'': 663,\n",
       " 'ולגביה': 664,\n",
       " 'הנארז': 665,\n",
       " 'המקור12': 666,\n",
       " 'מדירת': 667,\n",
       " 'שמבטיח': 668,\n",
       " 'פ\"מהרש': 669,\n",
       " 'בעפר': 670,\n",
       " 'שהפחתת': 671,\n",
       " 'ד1ב4': 672,\n",
       " 'טורקיה': 673,\n",
       " 'שיקולים–': 674,\n",
       " 'רכוש\"ל\"': 675,\n",
       " 'בּיכּר': 676,\n",
       " 'וליטף': 677,\n",
       " 'מאמור': 678,\n",
       " 'מההיסטוריה': 679,\n",
       " '\"רכב': 680,\n",
       " 'ולדודה': 681,\n",
       " 'שלתביעה': 682,\n",
       " 'שאהר': 683,\n",
       " 'שיקלו': 684,\n",
       " 'מבטי': 685,\n",
       " 'זרגוזי': 686,\n",
       " 'זידאן': 687,\n",
       " 'ממסירותה': 688,\n",
       " 'היחידהו': 689,\n",
       " 'המיוחסת\"': 690,\n",
       " 'שתמונותיהם': 691,\n",
       " 'ולתושבים': 692,\n",
       " 'ל\"שהנ': 693,\n",
       " 'נתגלע': 694,\n",
       " 'למיותר': 695,\n",
       " 'בפרמיות': 696,\n",
       " 'כשמתקיימים': 697,\n",
       " '10-9-כ': 698,\n",
       " '2018-ט\"התשע': 699,\n",
       " 'שעיכבו': 700,\n",
       " 'נזכור': 701,\n",
       " 'הרפואה': 702,\n",
       " 'שסיפחו': 703,\n",
       " 'רפעת': 704,\n",
       " 'שההגינות': 705,\n",
       " 'ולהשתקע': 706,\n",
       " 'שיוקם': 707,\n",
       " 'סלסילה': 708,\n",
       " 'בדירקטוריון': 709,\n",
       " 'אם37': 710,\n",
       " 'בקיץ': 711,\n",
       " 'מתועדת-בלתי': 712,\n",
       " 'שחייבת': 713,\n",
       " 'תכנה': 714,\n",
       " 'שרשמו': 715,\n",
       " 'הגשמתה': 716,\n",
       " 'כגישתו': 717,\n",
       " 'לקול\"': 718,\n",
       " 'מאת': 719,\n",
       " 'והמחקרים': 720,\n",
       " 'מלסווג': 721,\n",
       " 'קא': 722,\n",
       " 'כמוסכמת': 723,\n",
       " 'ובעיקר–': 724,\n",
       " 'המבקשים4': 725,\n",
       " 'שמשרדיה': 726,\n",
       " 'שזכאותו': 727,\n",
       " 'ומחדליהם': 728,\n",
       " 'ביטוחיים': 729,\n",
       " 'תכניתם': 730,\n",
       " 'לאשרו': 731,\n",
       " 'הסמוכה\"': 732,\n",
       " 'זעיר': 733,\n",
       " '\"מכח': 734,\n",
       " 'א57': 735,\n",
       " 'גבעוני': 736,\n",
       " 'סיור': 737,\n",
       " 'מסדירים\"': 738,\n",
       " 'נתפשה': 739,\n",
       " 'ומרחיק': 740,\n",
       " 'מלביני': 741,\n",
       " 'שיידו': 742,\n",
       " \"מסורתי'ה\": 743,\n",
       " 'השלכותיו': 744,\n",
       " '\"מהמלחמה': 745,\n",
       " 'המרשיע': 746,\n",
       " '1961–א\"התשכ': 747,\n",
       " '\"דיוני\\'': 748,\n",
       " 'תכנונית-האנטי': 749,\n",
       " 'לחדד': 750,\n",
       " 'העילות': 751,\n",
       " 'הפוסט': 752,\n",
       " 'בחמקמקות': 753,\n",
       " '\"מהן\\'': 754,\n",
       " 'פיצוטקה': 755,\n",
       " 'להעמידן': 756,\n",
       " 'בזכאות': 757,\n",
       " 'המתחרה': 758,\n",
       " 'מהותן': 759,\n",
       " '2010-התש״ע': 760,\n",
       " '\"ידוע': 761,\n",
       " 'ובמבנים': 762,\n",
       " 'ניתנה26': 763,\n",
       " 'ומילות': 764,\n",
       " \"'א'\": 765,\n",
       " 'לקשייהם': 766,\n",
       " 'הדיפת': 767,\n",
       " 'ובדרך\"': 768,\n",
       " 'ונעיר': 769,\n",
       " 'רישיונם': 770,\n",
       " 'הסיעוד': 771,\n",
       " 'שהתחוללה': 772,\n",
       " '\"העיר': 773,\n",
       " \"22קרא'\": 774,\n",
       " 'שנהפכה': 775,\n",
       " 'מלשאת': 776,\n",
       " 'פנימית\"': 777,\n",
       " 'נכרת': 778,\n",
       " 'שיטורי': 779,\n",
       " 'לתפוצה': 780,\n",
       " 'להתפתחות105': 781,\n",
       " 'רעל': 782,\n",
       " 'ולנסיבותיה': 783,\n",
       " '\"ילדים': 784,\n",
       " 'נִתַּק': 785,\n",
       " 'מיסויית': 786,\n",
       " \"'כלאם\": 787,\n",
       " 'ונישא': 788,\n",
       " 'פירעונן': 789,\n",
       " 'לפסילת7': 790,\n",
       " 'יגדל': 791,\n",
       " 'לגיליון': 792,\n",
       " 'וצלחות': 793,\n",
       " 'באזמל': 794,\n",
       " 'המס2': 795,\n",
       " 'מה-דבר\"': 796,\n",
       " 'שנִשא': 797,\n",
       " 'כדרכו': 798,\n",
       " 'פגומה': 799,\n",
       " 'וצביקה': 800,\n",
       " 'כוחנית': 801,\n",
       " 'בלבדית': 802,\n",
       " 'טוב-בר': 803,\n",
       " 'ומקורה': 804,\n",
       " 'למגבלות12': 805,\n",
       " 'וניהולן': 806,\n",
       " 'מחושבת': 807,\n",
       " 'אישרו': 808,\n",
       " \"'יומת\": 809,\n",
       " 'בשימושן': 810,\n",
       " 'טרחתו': 811,\n",
       " 'ולניסיון': 812,\n",
       " 'אלסייד': 813,\n",
       " 'בלובי': 814,\n",
       " 'מבחירתם': 815,\n",
       " 'שנפגעו': 816,\n",
       " 'ולעיכוב': 817,\n",
       " 'ועיכוב': 818,\n",
       " 'שנים-ארוכי': 819,\n",
       " 'אסי': 820,\n",
       " 'מנוח': 821,\n",
       " \"'פלוני\": 822,\n",
       " 'במסתננים': 823,\n",
       " 'שכותרות': 824,\n",
       " 'שפרסמו': 825,\n",
       " 'מוגברות': 826,\n",
       " 'ומגובשת': 827,\n",
       " 'לתחנוני': 828,\n",
       " 'כגישת': 829,\n",
       " '\"מיסודו': 830,\n",
       " '\"לגזענות': 831,\n",
       " 'תרופותיו': 832,\n",
       " 'ובידוד': 833,\n",
       " 'סמים5': 834,\n",
       " 'קלטות': 835,\n",
       " 'שביכר': 836,\n",
       " 'בב94': 837,\n",
       " 'המפעיל': 838,\n",
       " 'שנימק': 839,\n",
       " 'מאפיון': 840,\n",
       " 'והכנתם': 841,\n",
       " 'ההלכה\\uf0b7': 842,\n",
       " 'נפגע-\"': 843,\n",
       " 'אפשר-אי': 844,\n",
       " \"ובסיוע'\": 845,\n",
       " 'תקווה-פתח': 846,\n",
       " \"שבפקודת'\": 847,\n",
       " 'מהמדרגה': 848,\n",
       " '\"קורלנדר': 849,\n",
       " 'פליליות': 850,\n",
       " 'מראובן': 851,\n",
       " 'הכללתם': 852,\n",
       " 'מסוים30': 853,\n",
       " 'ותאריך': 854,\n",
       " 'להעברתם': 855,\n",
       " 'מהמעשים': 856,\n",
       " 'בד413': 857,\n",
       " 'בנשיאתו': 858,\n",
       " 'לנציגיהן': 859,\n",
       " 'ורצונה': 860,\n",
       " 'תורמוסעיא': 861,\n",
       " 'שנחקקה': 862,\n",
       " 'הצבעה': 863,\n",
       " 'לחשש\"': 864,\n",
       " 'מהתקף': 865,\n",
       " 'שוגר': 866,\n",
       " \"גריל'\": 867,\n",
       " 'זבת': 868,\n",
       " 'והתעריפים': 869,\n",
       " 'ובהפנותו': 870,\n",
       " 'חדשה\"': 871,\n",
       " 'נוער\"': 872,\n",
       " 'ההקצאות': 873,\n",
       " 'משביעת': 874,\n",
       " 'מגרעותיה': 875,\n",
       " 'החידושים': 876,\n",
       " 'בריאה': 877,\n",
       " 'ושוללים': 878,\n",
       " 'סיפוריה': 879,\n",
       " 'בוועדה\"': 880,\n",
       " 'טכנולוגיים': 881,\n",
       " 'ולהמרצת': 882,\n",
       " 'עודם': 883,\n",
       " 'שאנשיו': 884,\n",
       " 'ונענה': 885,\n",
       " 'הסתייג': 886,\n",
       " 'התבטל': 887,\n",
       " 'להפרזות': 888,\n",
       " 'בוחנים': 889,\n",
       " 'ב\"המצ': 890,\n",
       " 'שמותיהם': 891,\n",
       " '\"סנגורו': 892,\n",
       " 'משנתקיים': 893,\n",
       " 'שעורר': 894,\n",
       " 'לעאמר': 895,\n",
       " 'כמהנדס12': 896,\n",
       " 'ייצוגי-הצרכני': 897,\n",
       " 'פראן': 898,\n",
       " 'הינה': 899,\n",
       " '\"בכתב': 900,\n",
       " 'בוודאותו': 901,\n",
       " 'שניכרו': 902,\n",
       " 'העותרות\"': 903,\n",
       " 'סילוקן': 904,\n",
       " 'לעקר': 905,\n",
       " 'להתמצות': 906,\n",
       " 'ומטרידה': 907,\n",
       " 'נמנע15': 908,\n",
       " 'להפכו': 909,\n",
       " 'מבטחתה': 910,\n",
       " 'לפיכך29': 911,\n",
       " 'ובפרטיו': 912,\n",
       " 'ובעודם': 913,\n",
       " 'בדימ91': 914,\n",
       " 'לארוחת': 915,\n",
       " 'נסך': 916,\n",
       " 'והימנעותו': 917,\n",
       " '2-כ\"': 918,\n",
       " 'מגישות3': 919,\n",
       " 'ניאותה': 920,\n",
       " 'שנחזה': 921,\n",
       " 'פתיחה\"ל': 922,\n",
       " 'לרמות': 923,\n",
       " 'ומקבלת': 924,\n",
       " 'בדומה': 925,\n",
       " 'ומקובלנו': 926,\n",
       " 'וחישוב\"': 927,\n",
       " \"בסכסוך'\": 928,\n",
       " 'מנאע': 929,\n",
       " 'דהאידנא': 930,\n",
       " 'ארצי-בין15': 931,\n",
       " 'חיים\"ל': 932,\n",
       " 'בהעסקתם': 933,\n",
       " 'ובסמכות': 934,\n",
       " 'המתווים': 935,\n",
       " 'ארלוזורוב': 936,\n",
       " 'והפרכות': 937,\n",
       " 'למוֹד': 938,\n",
       " 'ענני': 939,\n",
       " 'לעובדה\"': 940,\n",
       " 'ובפיקוח': 941,\n",
       " 'ורחבות': 942,\n",
       " 'ובמאבק': 943,\n",
       " 'מנושי': 944,\n",
       " 'קדם\"': 945,\n",
       " '157ת': 946,\n",
       " 'ומדגישים': 947,\n",
       " 'הכספי': 948,\n",
       " '\"תמרה': 949,\n",
       " 'שיורים': 950,\n",
       " '1הרביעי': 951,\n",
       " 'בחובות': 952,\n",
       " '\"צבאי': 953,\n",
       " '20העליון': 954,\n",
       " 'השגות': 955,\n",
       " 'מבוגרים': 956,\n",
       " 'שהשקיעו': 957,\n",
       " 'שכה': 958,\n",
       " 'אמורפי': 959,\n",
       " 'לבחור': 960,\n",
       " 'מוודאות': 961,\n",
       " 'שהסנגורית': 962,\n",
       " 'א425': 963,\n",
       " 'בכך': 964,\n",
       " 'ייראו': 965,\n",
       " 'נכרתו': 966,\n",
       " 'ואזור': 967,\n",
       " '\"דחייתה': 968,\n",
       " 'ונייטרלי': 969,\n",
       " 'לסכנת': 970,\n",
       " 'המטאפורי': 971,\n",
       " '\"בנפש': 972,\n",
       " \"'עליה\": 973,\n",
       " 'ההענקה': 974,\n",
       " 'ולומר\"': 975,\n",
       " 'במשנתו': 976,\n",
       " 'ושחשיפתו': 977,\n",
       " \"אלקורד'\": 978,\n",
       " 'שנכתב': 979,\n",
       " 'לטביעות': 980,\n",
       " 'המצורף': 981,\n",
       " 'אביזר': 982,\n",
       " 'ובקוסובו': 983,\n",
       " 'מ\"תא-ב': 984,\n",
       " 'ועולות': 985,\n",
       " 'ויועמד': 986,\n",
       " 'אכרם': 987,\n",
       " 'מההורה': 988,\n",
       " \"קירח'\": 989,\n",
       " 'והתגלו': 990,\n",
       " 'שעד–': 991,\n",
       " 'ב332': 992,\n",
       " 'ר\"בכ': 993,\n",
       " 'דקלרטיבית': 994,\n",
       " 'בחריגוּת': 995,\n",
       " 'מזרז': 996,\n",
       " 'תודות': 997,\n",
       " 'סצנה': 998,\n",
       " 'התחרותיות': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(vocab_token2idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_idx(token):\n",
    "    if token in vocabulary:\n",
    "        return vocab_token2idx[token]\n",
    "    return len(vocabulary) - 1\n",
    "\n",
    "def get_word_from_idx(idx):\n",
    "    if idx > 0 and idx < len(vocabulary) - 1:\n",
    "        return vocab_idx2token[idx]\n",
    "    return vocab_idx2token[len(vocabulary) - 1]\n",
    "\n",
    "def get_words_from_encoded_sentence(indexes):\n",
    "    words = []\n",
    "    for index in indexes:\n",
    "        words.append(get_word_from_idx(index))\n",
    "    return words\n",
    "\n",
    "def get_indexes_from_words(tokens):\n",
    "    indexes = []\n",
    "    for token in tokens:\n",
    "        index = get_token_idx(token)\n",
    "        indexes.append(index)\n",
    "    return indexes\n",
    "\n",
    "def get_ner_idx(ner):\n",
    "    return reverse_ner_mapping[ner]\n",
    "\n",
    "def get_indexes_from_ners(ners):\n",
    "    indexes = []\n",
    "    for ner in ners:\n",
    "        index = get_ner_idx(ner)\n",
    "        indexes.append(index)\n",
    "    return indexes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19998, 19998, 5903, 19998, 19998, 19998, 19998, 19998, 19998, 19998] ['unknown', 'unknown', 'בבית', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx,batch in tqdm(enumerate(data_gen()),total=len(queries)):\n",
    "    batch = batch[batch['text'].notna()]\n",
    "    for index,row in batch.iterrows():\n",
    "        text = row['text']\n",
    "        tokens = get_raw_bag_of_words(text)\n",
    "        token = tokens[0:10]\n",
    "        i =get_indexes_from_words(token)\n",
    "        w = get_words_from_encoded_sentence(i)\n",
    "        print(i,w)\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:08<00:00,  4.10it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_dict = {\"tokens\":[], \"ner_tags\":[]}\n",
    "validation_data_dict = {\"tokens\":[], \"ner_tags\":[]}\n",
    "MAX_SEN_LENGTH = 25\n",
    "\n",
    "for idx,batch in tqdm(enumerate(data_gen()),total=len(queries)):\n",
    "    batch = batch[batch['text'].notna()]\n",
    "    for index,row in batch.iterrows():\n",
    "        text = row['text']\n",
    "        try:\n",
    "            sentences , ners_tags = create_sentences_tokens_ners_labels_batch(row,MAX_SEN_LENGTH)\n",
    "            split_index = int(len(sentences) * 0.8)\n",
    "\n",
    "            train_sentences = sentences[:split_index]\n",
    "            train_ner_tags = ners_tags[:split_index]\n",
    "\n",
    "            val_sentences = sentences[split_index:]\n",
    "            val_ner_tags = ners_tags[split_index:]\n",
    "\n",
    "\n",
    "            train_data_dict['tokens'].extend(train_sentences)\n",
    "            train_data_dict['ner_tags'].extend(train_ner_tags)\n",
    "\n",
    "            validation_data_dict['tokens'].extend(val_sentences)\n",
    "            validation_data_dict['ner_tags'].extend(val_ner_tags)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(data=train_data_dict)\n",
    "val_df = pd.DataFrame(data=validation_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197462"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_df = train_df[:8000]\n",
    "sub_val_df = train_df[:2000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_464637/3394779955.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_train_df['tokens'] = sub_train_df['tokens'].apply(lambda tokens: get_indexes_from_words(tokens))\n",
      "/tmp/ipykernel_464637/3394779955.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_val_df['tokens'] = sub_val_df['tokens'].apply(lambda tokens: get_indexes_from_words(tokens))\n"
     ]
    }
   ],
   "source": [
    "sub_train_df['tokens'] = sub_train_df['tokens'].apply(lambda tokens: get_indexes_from_words(tokens))\n",
    "sub_val_df['tokens'] = sub_val_df['tokens'].apply(lambda tokens: get_indexes_from_words(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_464637/4050277662.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_train_df['ner_tags'] = sub_train_df['ner_tags'].apply(lambda tokens: get_indexes_from_ners(tokens))\n",
      "/tmp/ipykernel_464637/4050277662.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_val_df['ner_tags'] = sub_val_df['ner_tags'].apply(lambda tokens: get_indexes_from_ners(tokens))\n"
     ]
    }
   ],
   "source": [
    "sub_train_df['ner_tags'] = sub_train_df['ner_tags'].apply(lambda tokens: get_indexes_from_ners(tokens))\n",
    "sub_val_df['ner_tags'] = sub_val_df['ner_tags'].apply(lambda tokens: get_indexes_from_ners(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[19998, 5923, 19998, 19998, 19998, 19998, 1999...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5923, 10430, 19998, 19998, 19998, 19998, 1999...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[19998, 19998, 19998, 18736, 19998, 19998, 199...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[19998, 19998, 19998, 19998, 16807, 3768, 5923...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[19998, 19998, 19998, 11107, 19998, 19998, 199...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [19998, 5923, 19998, 19998, 19998, 19998, 1999...   \n",
       "1  [5923, 10430, 19998, 19998, 19998, 19998, 1999...   \n",
       "2  [19998, 19998, 19998, 18736, 19998, 19998, 199...   \n",
       "3  [19998, 19998, 19998, 19998, 16807, 3768, 5923...   \n",
       "4  [19998, 19998, 19998, 11107, 19998, 19998, 199...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 0, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = len(list(ner_mapping.keys()))\n",
    "vocab_size = len(vocabulary)\n",
    "batch_size = 32\n",
    "epoch = 10\n",
    "embedding_dim = 100\n",
    "hidden_units = 64\n",
    "sequence_length = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_data = sub_train_df['tokens'].values\n",
    "train_output_data = sub_train_df['ner_tags'].values\n",
    "max_sequence_length = max(len(seq) for seq in train_input_data)\n",
    "\n",
    "if sequence_length <  max_sequence_length:\n",
    "    sequence_length = max_sequence_length\n",
    "\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_input_data, maxlen=max_sequence_length, padding='post')\n",
    "train_input_data_tensor = tf.convert_to_tensor(padded_sequences, dtype=tf.int32)\n",
    "max_sequence_length = max(len(seq) for seq in train_output_data)\n",
    "\n",
    "if sequence_length <  max_sequence_length:\n",
    "    sequence_length = max_sequence_length\n",
    "    \n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_output_data, maxlen=max_sequence_length, padding='post')\n",
    "train_output_data_tensor = tf.convert_to_tensor(padded_sequences, dtype=tf.int32)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_input_data_tensor, train_output_data_tensor))\n",
    "train_dataset = train_dataset.padded_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input_data = sub_val_df['tokens'].values\n",
    "val_output_data = sub_val_df['ner_tags'].values\n",
    "max_sequence_length = max(len(seq) for seq in val_input_data)\n",
    "if sequence_length <  max_sequence_length:\n",
    "    sequence_length = max_sequence_length\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(val_input_data, maxlen=max_sequence_length, padding='post')\n",
    "val_input_data_tensor = tf.convert_to_tensor(padded_sequences, dtype=tf.int32)\n",
    "\n",
    "max_sequence_length = max(len(seq) for seq in val_output_data)\n",
    "if sequence_length <  max_sequence_length:\n",
    "    sequence_length = max_sequence_length\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(val_output_data, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "\n",
    "\n",
    "val_output_data_tensor = tf.convert_to_tensor(padded_sequences, dtype=tf.int32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_input_data_tensor, val_output_data_tensor))\n",
    "val_dataset = val_dataset.padded_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 19999 27\n"
     ]
    }
   ],
   "source": [
    "print(num_tags,vocab_size,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = NERModel(num_tags,vocab_size, embed_dim=embedding_dim, num_heads=4, ff_dim=hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 13:17:52.029668: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-14 13:17:52.030875: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-14 13:17:52.031742: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_model(vocab_size, embedding_dim, hidden_units, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim),\n",
    "        layers.LSTM(hidden_units, return_sequences=True),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "rnn_model = create_rnn_model(vocab_size,embedding_dim,hidden_units,num_tags) \n",
    "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 13:23:02.272526: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-14 13:23:02.273610: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-14 13:23:02.274420: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-14 13:23:02.390127: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-14 13:23:02.391140: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-14 13:23:02.391973: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "def create_autoencoder_model(vocab_size, embedding_dim, hidden_units, sequence_length):\n",
    "    input_layer = layers.Input(shape=(sequence_length,))\n",
    "    embedding_layer = layers.Embedding(vocab_size, embedding_dim)(input_layer)\n",
    "    encoder = layers.LSTM(hidden_units)(embedding_layer)\n",
    "    decoder = layers.RepeatVector(sequence_length)(encoder)\n",
    "    decoder = layers.LSTM(hidden_units, return_sequences=True)(decoder)\n",
    "    output_layer = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))(decoder)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "autoencoder_model = create_autoencoder_model(vocab_size,embedding_dim,hidden_units,sequence_length)\n",
    "autoencoder_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 13:18:10.718934: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32 and shape [8000,27]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-14 13:18:10.719287: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [8000,27]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-07-14 13:18:10.857492: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-14 13:18:10.858433: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-14 13:18:10.859365: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-14 13:18:11.271593: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-14 13:18:11.272622: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-14 13:18:11.273272: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 5s 14ms/step - loss: 0.5015 - accuracy: 0.9273\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.1222 - accuracy: 0.9622\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.1038 - accuracy: 0.9647\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.0819 - accuracy: 0.9769\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.0834 - accuracy: 0.9748\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.0700 - accuracy: 0.9808\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.0695 - accuracy: 0.9808\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.0683 - accuracy: 0.9797\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.0655 - accuracy: 0.9822\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.0630 - accuracy: 0.9835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f942011d210>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(train_dataset, epochs=epoch, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 13:23:06.208245: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [8000,27]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-07-14 13:23:06.208543: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [8000,27]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-07-14 13:23:06.344885: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-14 13:23:06.345713: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-14 13:23:06.346405: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-14 13:23:06.459466: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-14 13:23:06.460736: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-14 13:23:06.461627: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-14 13:23:06.990648: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-14 13:23:06.991600: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-14 13:23:06.992282: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-14 13:23:07.099255: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-14 13:23:07.100099: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-14 13:23:07.100824: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 48s 183ms/step - loss: 2.0024\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 48s 190ms/step - loss: 0.4527\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 0.4444\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 50s 202ms/step - loss: 0.4427\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 48s 190ms/step - loss: 0.4420\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 0.4415\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 0.4411\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 49s 197ms/step - loss: 0.4408\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 0.4406\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.4404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f94207bee60>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_model.fit(train_dataset, epochs=epoch, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 01:06:02.385811: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [8000,27]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-07-14 01:06:02.386041: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [8000,27]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "/home/yuval/.local/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 3s 8ms/step - loss: 0.1032\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.0225\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.0218\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.0215\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.0205\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.0185\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.0177\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.0169\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.0161\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.0154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f904c662da0>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = CustomNonPaddingTokenLoss()\n",
    "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
    "ner_model.fit(train_dataset, epochs=epoch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare (2) and (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_preds(dataset):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        output = ner_model.predict(x)\n",
    "        predictions = np.argmax(output, axis=-1)\n",
    "        predictions = np.reshape(predictions, [-1])\n",
    "\n",
    "        true_tag_ids = np.reshape(y, [-1])\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "\n",
    "    predicted_tags = [ner_mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [ner_mapping[tag] for tag in all_true_tag_ids]\n",
    "    return real_tags,predicted_tags\n",
    "\n",
    "real_tags,predicted_tags = get_preds(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = 0\n",
    "negative = 0\n",
    "for i in range(len(real_tags)):\n",
    "    if real_tags[i] == predicted_tags[i]:\n",
    "        positive += 1\n",
    "    else:\n",
    "        negative +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Of Correct Token NER Predictions :  49775\n",
      "Num Of Wrong Token NER Predictions :  103\n"
     ]
    }
   ],
   "source": [
    "print(\"Num Of Correct Token NER Predictions : \", positive)\n",
    "print(\"Num Of Wrong Token NER Predictions : \", negative)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Use RNN to peform sentiment analysis on each verdict,  add sentiment Column for dataset (POSITIVE NETURAL NEGETIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuval/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated, use `top_k=1` if you want similar functionnality\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"avichr/heBERT_sentiment_analysis\",\n",
    "    tokenizer=\"avichr/heBERT_sentiment_analysis\",\n",
    "    return_all_scores = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_input_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text:str):\n",
    "    positive_c = 20\n",
    "    neutral_c = 10\n",
    "    negative_c = 0.1\n",
    "    rest = se_input_size - len(text) % se_input_size\n",
    "    num_iterations = int(len(text) / se_input_size) - 1\n",
    "    negative = 0\n",
    "    positive = 0\n",
    "    neutral = 0\n",
    "    start = 0\n",
    "    end = se_input_size + 1\n",
    "    for i in range(num_iterations):\n",
    "        pos,net,neg = get_sub_sentiment(text,start,end)\n",
    "        start += se_input_size\n",
    "        end += se_input_size\n",
    "        negative += neg\n",
    "        neutral += net\n",
    "        positive += pos\n",
    "\n",
    "    if rest > 0:\n",
    "        pos,net,neg = get_sub_sentiment(text,len(text) - 1 - rest ,len(text) - 1)\n",
    "        negative += neg\n",
    "        neutral += net\n",
    "        positive += pos\n",
    "\n",
    "    negative = negative * negative_c / num_iterations\n",
    "    positive = positive * positive_c / num_iterations\n",
    "    neutral = neutral * neutral_c / num_iterations\n",
    "    \n",
    "    output = None\n",
    "    if negative > positive and negative > neutral:\n",
    "        output =  \"NEGATIVE\"\n",
    "    if positive > negative and positive > neutral:\n",
    "        output = \"POSITIVE\"\n",
    "    if neutral > negative and neutral > positive:\n",
    "        output = \"NEUTRAL\"\n",
    "\n",
    "    print(\"OUTPUT:\",output,\"| positive:\",positive,\"| negative:\",negative,\"| neutral:\",neutral)\n",
    "    return output\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def get_sub_sentiment(text:str,start,end):\n",
    "    t = text[start:end]\n",
    "    se = sentiment_analysis(t)\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    net = 0    \n",
    "    for score in se:\n",
    "        if score['label'] == \"positive\":\n",
    "            pos = score['score']\n",
    "        if score['label'] == \"negative\":\n",
    "            neg = score['score']\n",
    "        if score['label'] == \"neutral\":\n",
    "            net = score['score']\n",
    "\n",
    "    return pos,net,neg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: NEUTRAL | positive: 0.1285815097666769 | negative: 0.08775837236687288 | neutral: 1.6144175741812268\n",
      "OUTPUT: NEUTRAL | positive: 0.029250732663382618 | negative: 0.09616774313963287 | neutral: 0.41810522705328357\n",
      "OUTPUT: NEUTRAL | positive: 0.07245724404070174 | negative: 0.0847925697428441 | neutral: 1.6458046722747837\n",
      "OUTPUT: NEUTRAL | positive: 0.26931737055265864 | negative: 0.08991059120778706 | neutral: 1.4625173504507127\n",
      "OUTPUT: NEUTRAL | positive: 0.07607056328686335 | negative: 0.07905551300093693 | neutral: 2.110176840247458\n",
      "OUTPUT: NEUTRAL | positive: 0.09803263049910865 | negative: 0.09740595984206137 | neutral: 0.3294353143965487\n",
      "OUTPUT: NEUTRAL | positive: 0.015704079719615124 | negative: 0.09929496742173335 | neutral: 0.221381326869411\n",
      "OUTPUT: NEUTRAL | positive: 0.07262592645669688 | negative: 0.0935841753643347 | neutral: 0.9624123780278231\n",
      "OUTPUT: NEUTRAL | positive: 0.18214774045382479 | negative: 0.09190427225686634 | neutral: 1.244814711558559\n",
      "OUTPUT: NEUTRAL | positive: 0.1581761444301661 | negative: 0.09318732720154983 | neutral: 0.653461222202541\n",
      "OUTPUT: NEUTRAL | positive: 0.0828844173354975 | negative: 0.09820376187562943 | neutral: 0.5927269662639777\n",
      "OUTPUT: NEUTRAL | positive: 0.05132907202782359 | negative: 0.09507128570214518 | neutral: 0.5549262094354434\n",
      "OUTPUT: NEUTRAL | positive: 0.05092508891923191 | negative: 0.09563357977466594 | neutral: 0.5500683184067788\n",
      "OUTPUT: NEUTRAL | positive: 0.10519574415047828 | negative: 0.09478078350424766 | neutral: 0.7193238412655774\n",
      "OUTPUT: NEUTRAL | positive: 0.01141450168688607 | negative: 0.09920821273080947 | neutral: 0.6984714276723025\n",
      "OUTPUT: NEUTRAL | positive: 0.09939340762007595 | negative: 0.08373489406328596 | neutral: 1.6300053500371894\n",
      "OUTPUT: NEUTRAL | positive: 0.05921827268488974 | negative: 0.09698716534991177 | neutral: 0.46398192043432995\n",
      "OUTPUT: NEUTRAL | positive: 0.015773430340442037 | negative: 0.09488820444183568 | neutral: 0.6956006661759881\n",
      "OUTPUT: NEUTRAL | positive: 0.035591058959570626 | negative: 0.09913287281016397 | neutral: 0.2507354163013092\n",
      "OUTPUT: NEUTRAL | positive: 0.19831275884196378 | negative: 0.0780832138002062 | neutral: 2.8617530427156734\n",
      "OUTPUT: NEUTRAL | positive: 0.09267062445360352 | negative: 0.08470214518407981 | neutral: 1.9001167933068548\n",
      "OUTPUT: NEUTRAL | positive: 0.17650296562351286 | negative: 0.09536803385206298 | neutral: 0.5710235619135703\n",
      "OUTPUT: NEUTRAL | positive: 0.20681253526337504 | negative: 0.06692000991872889 | neutral: 3.4173586375502363\n",
      "OUTPUT: NEUTRAL | positive: 0.16507569642271847 | negative: 0.09444173912462943 | neutral: 1.0615234917863479\n",
      "OUTPUT: NEUTRAL | positive: 0.16251064007091748 | negative: 0.08496894473004539 | neutral: 1.71596790317006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/36 [04:53<2:51:20, 293.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: NEUTRAL | positive: 0.010840320601025322 | negative: 0.09885795071584119 | neutral: 0.22643183370553957\n",
      "OUTPUT: NEUTRAL | positive: 0.08268343102827203 | negative: 0.0900664607195982 | neutral: 1.6662978181245438\n",
      "OUTPUT: NEUTRAL | positive: 0.09370064465085391 | negative: 0.07173344011900427 | neutral: 2.837945183454458\n",
      "OUTPUT: NEUTRAL | positive: 0.07607056328686335 | negative: 0.07905551300093693 | neutral: 2.110176840247458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/36 [05:50<3:24:16, 350.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: NEUTRAL | positive: 0.14908602357931872 | negative: 0.08101004180346576 | neutral: 2.0418441819271287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m index,row \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m      4\u001b[0m     text \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m     row[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m get_sentiment(text)\n",
      "Cell \u001b[0;32mIn[88], line 26\u001b[0m, in \u001b[0;36mget_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     23\u001b[0m     neutral \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m net\n\u001b[1;32m     24\u001b[0m     positive \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pos\n\u001b[0;32m---> 26\u001b[0m negative \u001b[39m=\u001b[39m negative \u001b[39m*\u001b[39;49m negative_c \u001b[39m/\u001b[39;49m num_iterations\n\u001b[1;32m     27\u001b[0m positive \u001b[39m=\u001b[39m positive \u001b[39m*\u001b[39m positive_c \u001b[39m/\u001b[39m num_iterations\n\u001b[1;32m     28\u001b[0m neutral \u001b[39m=\u001b[39m neutral \u001b[39m*\u001b[39m neutral_c \u001b[39m/\u001b[39m num_iterations\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "for idx,batch in tqdm(enumerate(data_gen()),total=len(queries)):\n",
    "    batch = batch[batch['text'].notna()]\n",
    "    for index,row in batch.iterrows():\n",
    "        text = row['text']\n",
    "        row['sentiment'] = get_sentiment(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dataset Statistics with extra stats:\n",
    "    - number of verdicts each judge gave for every sentiment type\n",
    "    - In Certain TimeFrame: num of verdicts all judges gave for every sentiment type  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2004-01-01\"\n",
    "end_date = \"2005-01-01\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 2-5 subject tags for each verdict using AUTOENCODER  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder_model(vocab_size, embedding_dim, hidden_dim):\n",
    "    input_layer = layers.Input(shape=(None,))\n",
    "    embedding_layer = layers.Embedding(vocab_size, embedding_dim)(input_layer)\n",
    "    encoder = layers.LSTM(hidden_dim)(embedding_layer)\n",
    "    decoder = layers.RepeatVector(tf.shape(embedding_layer)[1])(encoder)\n",
    "    decoder = layers.LSTM(hidden_dim, return_sequences=True)(decoder)\n",
    "    output_layer = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))(decoder)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "hidden_dim = 64\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder_model = create_autoencoder_model(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the model on your text data\n",
    "autoencoder_model.fit(train_data, train_data, epochs=10, batch_size=32)\n",
    "\n",
    "# Encode the input text and decode the generated embedding\n",
    "input_text = \"Sample input text\"\n",
    "encoded_text = autoencoder_model.encoder.predict(input_text)\n",
    "decoded_text = autoencoder_model.decoder.predict(encoded_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Compare (3), (4) , (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
